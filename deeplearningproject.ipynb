{
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "ILM16xV1PD8m",
        "bqVZBxrvPD8n",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "ZwW2pl1O3WwT",
        "NXm0OV-S9E2g",
        "Q5pG0ENk5O63",
        "M0QVFX0B8C8x",
        "jNouVC-U62KI",
        "0NNA-0cy-dcQ",
        "j3CETLqIAO6f",
        "4YbDiZh7A-_O",
        "EzCh2lklBEsf",
        "kmPcFk5iFH6C",
        "XYy5gcchKnaH",
        "h7gi-LRXLeZl",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "xJivPyE8q_2k",
        "Of3PJYNbrGff",
        "jbkhcZqdrGfh",
        "riLp7y9brHca",
        "1Qw8mtL3rHcd",
        "I3aNiIh9fKqZ",
        "hTF1e9PwfKqa",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "jLrtcv83OEmf",
        "DADo_qtwOEmg",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "pEMng2IbBLp7",
        "TNVZ9zx19K6k",
        "A1tXasLFv6Dt",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "OeJFEK0N496M",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "peAK6Cc_HQeo",
        "khncImPpHcol",
        "wTe8K5rdYGV1",
        "yyzM232tatvC",
        "OMC09DFCbrEm",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "gpuType": "V28",
      "cell_execution_strategy": "setup",
      "name": "deeplearningproject",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "TPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 8481205,
          "sourceType": "datasetVersion",
          "datasetId": 5058629
        }
      ],
      "dockerImageVersionId": 30699,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VargheseTito/E-Commerce-Customer-Satisfaction-Score-Prediction-DL-Model/blob/main/deeplearningproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'ecomdata:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5058629%2F8481205%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240603%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240603T053520Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0e77030e4c12c48f3d44329007c054ca7bfde34ddba8e1ce742d328e7a6487bd2c997d66738b70c389e7974a39e826dc41b1dc7558e1e3f898ae1ffb253ee417fdece758525b3774e588b51a10beba1e3445207291420acdfd0fb1372b6c9ddc23152c554d138340cc14df8232168d3c7c0eaf26d31b1e31a2abfbca7e964cf10d689d6148af3a2d0bbb1d8d527817229f71e579d2f0ccd61d8f3a2d301ddaf97db883938c5d9f455bacd4e91e84801c8d4fb2255caf384738cc7311f48eecea6a12d97ade0f9d5246b9310a83b98f800892ca2d700c9a235a58a1362cdd6b4c597dfd6b86774e85ee220da65910d09b4f872e7d62ac96088c245bd97e39916b'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "DTEOchVumGHb"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  **E-Commerce Customer Satisfaction Score Prediction**\n",
        "\n"
      ],
      "metadata": {
        "id": "ILM16xV1PD8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "bqVZBxrvPD8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BUSINESS PROBLEM OVERVIEW**\n",
        "\n",
        "\n",
        "Customer satisfaction in the e-commerce sector is a pivotal metric that influences loyalty, repeat business, and word-of-mouth marketing. Traditionally, companies have relied on direct surveys to gauge customer satisfaction, which can be time-consuming and may not always capture the full spectrum of customer experiences. With the advent of deep learning, it's now possible to predict customer satisfaction scores in real-time, offering a granular view of service performance and identifying areas for immediate improvement.\n",
        "\n",
        "**Project Goal**\n",
        "\n",
        "The primary goal of this project is to develop a deep learning model that can accurately predict the CSAT scores based on customer interactions and feedback. By doing so, we aim to provide e-commerce businesses with a powerful tool to monitor and enhance customer satisfaction in real-time, thereby improving service quality and fostering customer loyalty."
      ],
      "metadata": {
        "id": "jivUA-ONOwse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Data manipulation and visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine learning and ANN building\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:48.921035Z",
          "iopub.execute_input": "2024-06-03T03:41:48.921967Z",
          "iopub.status.idle": "2024-06-03T03:41:51.126487Z",
          "shell.execute_reply.started": "2024-06-03T03:41:48.921934Z",
          "shell.execute_reply": "2024-06-03T03:41:51.125582Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading E-Commerce Dataset in pandas dataframe\n",
        "dataset=pd.read_csv(\"/kaggle/input/ecomdata/eCommerce_Customer_support_data.csv\")\n"
      ],
      "metadata": {
        "id": "FQ9iULLgVgv0",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:51.128623Z",
          "iopub.execute_input": "2024-06-03T03:41:51.129575Z",
          "iopub.status.idle": "2024-06-03T03:41:51.882093Z",
          "shell.execute_reply.started": "2024-06-03T03:41:51.129541Z",
          "shell.execute_reply": "2024-06-03T03:41:51.881251Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:51.883325Z",
          "iopub.execute_input": "2024-06-03T03:41:51.883722Z",
          "iopub.status.idle": "2024-06-03T03:41:51.921585Z",
          "shell.execute_reply.started": "2024-06-03T03:41:51.883688Z",
          "shell.execute_reply": "2024-06-03T03:41:51.920753Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns\n",
        "dataset.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:51.923493Z",
          "iopub.execute_input": "2024-06-03T03:41:51.923776Z",
          "iopub.status.idle": "2024-06-03T03:41:51.928906Z",
          "shell.execute_reply.started": "2024-06-03T03:41:51.923753Z",
          "shell.execute_reply": "2024-06-03T03:41:51.927988Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "dataset.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:51.930191Z",
          "iopub.execute_input": "2024-06-03T03:41:51.930477Z",
          "iopub.status.idle": "2024-06-03T03:41:52.086398Z",
          "shell.execute_reply.started": "2024-06-03T03:41:51.930448Z",
          "shell.execute_reply": "2024-06-03T03:41:52.085197Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "len(dataset[dataset.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:52.087776Z",
          "iopub.execute_input": "2024-06-03T03:41:52.088629Z",
          "iopub.status.idle": "2024-06-03T03:41:52.273211Z",
          "shell.execute_reply.started": "2024-06-03T03:41:52.08859Z",
          "shell.execute_reply": "2024-06-03T03:41:52.272371Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(dataset.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:52.27418Z",
          "iopub.execute_input": "2024-06-03T03:41:52.274465Z",
          "iopub.status.idle": "2024-06-03T03:41:52.400452Z",
          "shell.execute_reply.started": "2024-06-03T03:41:52.274441Z",
          "shell.execute_reply": "2024-06-03T03:41:52.399586Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "\n",
        "# Step 1: Calculate the count of missing values in each column and sort in descending order\n",
        "missing_values = dataset.isnull().sum().sort_values(ascending=False)\n",
        "\n",
        "\n",
        "# Step 2: Create a horizontal bar plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(x=missing_values, y=missing_values.index, orient='h')\n",
        "plt.xlabel('Count of Missing Values')\n",
        "plt.ylabel('Columns')\n",
        "plt.title('Count of Missing Values in Each Column')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bbvXMLwbnbjF",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:52.401975Z",
          "iopub.execute_input": "2024-06-03T03:41:52.40242Z",
          "iopub.status.idle": "2024-06-03T03:41:53.050351Z",
          "shell.execute_reply.started": "2024-06-03T03:41:52.402387Z",
          "shell.execute_reply": "2024-06-03T03:41:53.049451Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset given is a dataset from E-Commerce industry, and we have to analysis the customers satisfaction score and the insights behind it.\n",
        "\n",
        "Customer Satisfaction Score (CSAT) is a key performance indicator (KPI) used to gauge the level of satisfaction customers have with a company's products, services, or overall experience. In the context of e-commerce, CSAT typically measures how happy customers are with their online shopping experience, including aspects like product quality, website usability, delivery speed, and customer service.\n",
        "\n",
        "CSAT is an essential metric for e-commerce businesses, as it directly reflects the customers' perceptions and experiences, driving both immediate and long-term business success.\n",
        "\n",
        "The above dataset has 85907 rows and 20 columns. There are no duplicate values in the dataset, but there are  mising values in a few columns such as Customer_city,Product_category,item_price,order_id,order_date_time,customer remarks and connected_handling_time.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "dataset.columns"
      ],
      "metadata": {
        "id": "n87BaXA_42-R",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:53.051516Z",
          "iopub.execute_input": "2024-06-03T03:41:53.051808Z",
          "iopub.status.idle": "2024-06-03T03:41:53.057787Z",
          "shell.execute_reply.started": "2024-06-03T03:41:53.051784Z",
          "shell.execute_reply": "2024-06-03T03:41:53.056901Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "dataset.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:53.06201Z",
          "iopub.execute_input": "2024-06-03T03:41:53.062331Z",
          "iopub.status.idle": "2024-06-03T03:41:53.541987Z",
          "shell.execute_reply.started": "2024-06-03T03:41:53.062308Z",
          "shell.execute_reply": "2024-06-03T03:41:53.541074Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unique id:** Unique identifier for each record (integer).\n",
        "\n",
        "**Channel name:** Name of the customer service channel (object/string),3 unique channel name.\n",
        "\n",
        "**Category:** Category of the interaction (object/string) ,12 unique category.\n",
        "\n",
        "**Sub-category:** Sub-category of the interaction (object/string),57 unique sub-category.\n",
        "\n",
        "**Customer Remarks:** Feedback provided by the customer (object/string).\n",
        "\n",
        "**Order id:** Identifier for the order associated with the interaction (integer).\n",
        "\n",
        "**Order date time:** Date and time of the order (datetime).\n",
        "\n",
        "**Issue reported at:** Timestamp when the issue was reported (datetime).\n",
        "\n",
        "**Issue responded:** Timestamp when the issue was responded to (datetime).\n",
        "\n",
        "**Survey response date:** Date of the customer survey response (datetime).\n",
        "\n",
        "**Customer city:** City of the customer (object/string),1782 unique Customer city.\n",
        "\n",
        "**Product category:** Category of the product (object/string),9 unique product category.\n",
        "\n",
        "**Item price:** Price of the item (float).\n",
        "\n",
        "**Connected handling time:** Time taken to handle the interaction (float).\n",
        "\n",
        "**Agent name:** Name of the customer service agent (object/string),1371 unique agent name.\n",
        "\n",
        "**Supervisor:** Name of the supervisor (object/string),40 unique Supervisor.\n",
        "\n",
        "**Manager:** Name of the manager (object/string),6 unique manager.\n",
        "\n",
        "**Tenure Bucket:** Bucket categorizing agent tenure (object/string).\n",
        "\n",
        "**Agent Shift:** Shift timing of the agent (object/string).\n",
        "\n",
        "**CSAT Score:** Customer Satisfaction (CSAT) score (integer) (Target-Variable)."
      ],
      "metadata": {
        "id": "lGK0kiCdrdEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in dataset.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",dataset[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:53.543197Z",
          "iopub.execute_input": "2024-06-03T03:41:53.543477Z",
          "iopub.status.idle": "2024-06-03T03:41:53.708054Z",
          "shell.execute_reply.started": "2024-06-03T03:41:53.543454Z",
          "shell.execute_reply": "2024-06-03T03:41:53.707159Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Exploratory Data Analaysis (Data Wrangling)***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Create a copy of the current dataset and assigning to df\n",
        "df=dataset.copy()\n",
        "# Checking Shape of True Value\n",
        "print(\"No. of customers interaction and feedbacks with highest customer satisfaction scores  :\",len(df[df['CSAT Score']==5]))\n",
        "# Assigning  customers data to variable df_best_score\n",
        "df_best_score=df[(df['CSAT Score']==5)]\n",
        "df_least_score=df[(df['CSAT Score']==1)]"
      ],
      "metadata": {
        "id": "hf8BBIXE22e4",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:53.709265Z",
          "iopub.execute_input": "2024-06-03T03:41:53.709547Z",
          "iopub.status.idle": "2024-06-03T03:41:53.762499Z",
          "shell.execute_reply.started": "2024-06-03T03:41:53.709524Z",
          "shell.execute_reply": "2024-06-03T03:41:53.761584Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q1. Top 5 Product Category with highest Customer Satisfaction Score**"
      ],
      "metadata": {
        "id": "ZwW2pl1O3WwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Groupby Product_category Wise w.r.t Customer satisfaction score data\n",
        "grouped_df = df_best_score.groupby('Product_category').agg(\n",
        "    Count=('CSAT Score', 'size')\n",
        ").sort_values(by='Count',ascending=False)\n",
        "\n",
        "grouped_df[:5]"
      ],
      "metadata": {
        "id": "Ek9jajxU45JV",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:53.763881Z",
          "iopub.execute_input": "2024-06-03T03:41:53.764499Z",
          "iopub.status.idle": "2024-06-03T03:41:53.788985Z",
          "shell.execute_reply.started": "2024-06-03T03:41:53.764463Z",
          "shell.execute_reply": "2024-06-03T03:41:53.788013Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the data\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "grouped_df[:5].plot(kind='bar', ax=ax)\n",
        "\n",
        "# Adding labels and title\n",
        "ax.set_xlabel('Product Category')\n",
        "ax.set_ylabel('Count of CSAT Scores')\n",
        "ax.set_title('Top 5 Product Category with highest Customer Satisfaction Score')\n",
        "\n",
        "# Rotating the x-axis labels for better readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Adding grid for better readability\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bGBJmOe_4Qs0",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:53.790242Z",
          "iopub.execute_input": "2024-06-03T03:41:53.7913Z",
          "iopub.status.idle": "2024-06-03T03:41:54.139743Z",
          "shell.execute_reply.started": "2024-06-03T03:41:53.791266Z",
          "shell.execute_reply": "2024-06-03T03:41:54.138795Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q2. Top 5 Category with highest customer satisfaction score**"
      ],
      "metadata": {
        "id": "NXm0OV-S9E2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Groupby Customer_City Wise w.r.t Customer satisfaction score data\n",
        "grouped_df = df_best_score.groupby('category').agg(\n",
        "    Count=('CSAT Score', 'size')\n",
        ").sort_values(by='Count',ascending=False)\n",
        "\n",
        "grouped_df[:5]"
      ],
      "metadata": {
        "id": "TQ-wDXYs9Qhj",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:54.140918Z",
          "iopub.execute_input": "2024-06-03T03:41:54.141207Z",
          "iopub.status.idle": "2024-06-03T03:41:54.161847Z",
          "shell.execute_reply.started": "2024-06-03T03:41:54.141183Z",
          "shell.execute_reply": "2024-06-03T03:41:54.160981Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Plotting the data\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "grouped_df[:5].plot(kind='bar', ax=ax)\n",
        "\n",
        "# Adding labels and title\n",
        "ax.set_xlabel('Category')\n",
        "ax.set_ylabel('Count of CSAT Scores')\n",
        "ax.set_title('Top 5 Categories with highest Customer Satisfaction Score')\n",
        "\n",
        "# Rotating the x-axis labels for better readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Adding grid for better readability\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_ihGFAML9goW",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:54.162869Z",
          "iopub.execute_input": "2024-06-03T03:41:54.163202Z",
          "iopub.status.idle": "2024-06-03T03:41:54.420018Z",
          "shell.execute_reply.started": "2024-06-03T03:41:54.163169Z",
          "shell.execute_reply": "2024-06-03T03:41:54.419014Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q3. Top 5 Sub Category with highest Customer Satisfaction Score**"
      ],
      "metadata": {
        "id": "Q5pG0ENk5O63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Groupby Sub-category Wise w.r.t Customer satisfaction score data\n",
        "grouped_df = df_best_score.groupby('Sub-category').agg(\n",
        "    Count=('CSAT Score', 'size')\n",
        ").sort_values(by='Count',ascending=False)\n",
        "\n",
        "grouped_df[:5]"
      ],
      "metadata": {
        "id": "NLpBbO8q8fBO",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:54.421737Z",
          "iopub.execute_input": "2024-06-03T03:41:54.422082Z",
          "iopub.status.idle": "2024-06-03T03:41:54.444434Z",
          "shell.execute_reply.started": "2024-06-03T03:41:54.422057Z",
          "shell.execute_reply": "2024-06-03T03:41:54.443533Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the data\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "grouped_df[:5].plot(kind='bar', ax=ax)\n",
        "\n",
        "# Adding labels and title\n",
        "ax.set_xlabel('Sub Category')\n",
        "ax.set_ylabel('Count of CSAT Scores')\n",
        "ax.set_title('Top 5 Sub Category with highest Customer Satisfaction Score')\n",
        "\n",
        "# Rotating the x-axis labels for better readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Adding grid for better readability\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-CmN_LSP547X",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:54.445487Z",
          "iopub.execute_input": "2024-06-03T03:41:54.445804Z",
          "iopub.status.idle": "2024-06-03T03:41:54.785083Z",
          "shell.execute_reply.started": "2024-06-03T03:41:54.445779Z",
          "shell.execute_reply": "2024-06-03T03:41:54.784184Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q4. Top 5 cities with highest customer satisfaction score**"
      ],
      "metadata": {
        "id": "M0QVFX0B8C8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Groupby Customer_City Wise w.r.t Customer satisfaction score data\n",
        "grouped_df = df_best_score.groupby('Customer_City').agg(\n",
        "    Count=('CSAT Score', 'size')\n",
        ").sort_values(by='Count',ascending=False)\n",
        "\n",
        "grouped_df[:5]"
      ],
      "metadata": {
        "id": "0JBLdnPH6re2",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:54.786374Z",
          "iopub.execute_input": "2024-06-03T03:41:54.787121Z",
          "iopub.status.idle": "2024-06-03T03:41:54.810983Z",
          "shell.execute_reply.started": "2024-06-03T03:41:54.787082Z",
          "shell.execute_reply": "2024-06-03T03:41:54.810014Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the data\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "grouped_df[:5].plot(kind='bar', ax=ax)\n",
        "\n",
        "# Adding labels and title\n",
        "ax.set_xlabel('Cities')\n",
        "ax.set_ylabel('Count of CSAT Scores')\n",
        "ax.set_title('Top 5 Cities with highest Customer Satisfaction Score')\n",
        "\n",
        "# Rotating the x-axis labels for better readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Adding grid for better readability\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j1_fDvIT83uB",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:54.812086Z",
          "iopub.execute_input": "2024-06-03T03:41:54.812341Z",
          "iopub.status.idle": "2024-06-03T03:41:55.141582Z",
          "shell.execute_reply.started": "2024-06-03T03:41:54.812321Z",
          "shell.execute_reply": "2024-06-03T03:41:55.14067Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q5. Best performing Channels**"
      ],
      "metadata": {
        "id": "jNouVC-U62KI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Groupby Channel name Wise w.r.t Customer satisfaction score data\n",
        "grouped_df = df_best_score.groupby('channel_name').agg(\n",
        "    Count=('CSAT Score', 'size')\n",
        ").sort_values(by='Count',ascending=False)\n",
        "\n",
        "grouped_df"
      ],
      "metadata": {
        "id": "Zv9cJLcI8JGt",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:55.142791Z",
          "iopub.execute_input": "2024-06-03T03:41:55.143089Z",
          "iopub.status.idle": "2024-06-03T03:41:55.162171Z",
          "shell.execute_reply.started": "2024-06-03T03:41:55.143054Z",
          "shell.execute_reply": "2024-06-03T03:41:55.161279Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the data\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "grouped_df[:5].plot(kind='bar', ax=ax)\n",
        "\n",
        "# Adding labels and title\n",
        "ax.set_xlabel('Channels')\n",
        "ax.set_ylabel('Count of CSAT Scores')\n",
        "ax.set_title('Channel with highest Customer Satisfaction Score')\n",
        "\n",
        "# Rotating the x-axis labels for better readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Adding grid for better readability\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ttetFGHr6qlU",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:55.163379Z",
          "iopub.execute_input": "2024-06-03T03:41:55.163757Z",
          "iopub.status.idle": "2024-06-03T03:41:55.452574Z",
          "shell.execute_reply.started": "2024-06-03T03:41:55.163725Z",
          "shell.execute_reply": "2024-06-03T03:41:55.451691Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q6. Top 3 best performing Managers**"
      ],
      "metadata": {
        "id": "0NNA-0cy-dcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Groupby Manager Wise w.r.t Customer satisfaction score data\n",
        "grouped_df = df_best_score.groupby('Manager').agg(\n",
        "    Count=('CSAT Score', 'size')\n",
        ").sort_values(by='Count',ascending=False)\n",
        "\n",
        "grouped_df[:3]"
      ],
      "metadata": {
        "id": "YNCHUNGA-fKY",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:55.453985Z",
          "iopub.execute_input": "2024-06-03T03:41:55.454673Z",
          "iopub.status.idle": "2024-06-03T03:41:55.477243Z",
          "shell.execute_reply.started": "2024-06-03T03:41:55.454615Z",
          "shell.execute_reply": "2024-06-03T03:41:55.476387Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the data\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "grouped_df[:3].plot(kind='bar', ax=ax)\n",
        "\n",
        "# Adding labels and title\n",
        "ax.set_xlabel('Manager Name')\n",
        "ax.set_ylabel('Count of CSAT Scores')\n",
        "ax.set_title('Managers with highest Customer Satisfaction Score')\n",
        "\n",
        "# Rotating the x-axis labels for better readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Adding grid for better readability\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ceinX08y_FL4",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:55.478617Z",
          "iopub.execute_input": "2024-06-03T03:41:55.478983Z",
          "iopub.status.idle": "2024-06-03T03:41:55.79069Z",
          "shell.execute_reply.started": "2024-06-03T03:41:55.478956Z",
          "shell.execute_reply": "2024-06-03T03:41:55.78981Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q7. Top 3 best performing Agents**"
      ],
      "metadata": {
        "id": "j3CETLqIAO6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Groupby Agent_name Wise w.r.t Customer satisfaction score data\n",
        "grouped_df = df_best_score.groupby('Agent_name').agg(\n",
        "    Count=('CSAT Score', 'size')\n",
        ").sort_values(by='Count',ascending=False)\n",
        "\n",
        "grouped_df[:3]"
      ],
      "metadata": {
        "id": "w7O1lQ-2_eK_",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:55.791975Z",
          "iopub.execute_input": "2024-06-03T03:41:55.792615Z",
          "iopub.status.idle": "2024-06-03T03:41:55.816905Z",
          "shell.execute_reply.started": "2024-06-03T03:41:55.792581Z",
          "shell.execute_reply": "2024-06-03T03:41:55.816104Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the data\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "grouped_df[:3].plot(kind='bar', ax=ax)\n",
        "\n",
        "# Adding labels and title\n",
        "ax.set_xlabel('Agent Name')\n",
        "ax.set_ylabel('Count of CSAT Scores')\n",
        "ax.set_title('Agents with highest Customer Satisfaction Score')\n",
        "\n",
        "# Rotating the x-axis labels for better readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Adding grid for better readability\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lWt0erua_plf",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:55.817945Z",
          "iopub.execute_input": "2024-06-03T03:41:55.818235Z",
          "iopub.status.idle": "2024-06-03T03:41:56.055872Z",
          "shell.execute_reply.started": "2024-06-03T03:41:55.818211Z",
          "shell.execute_reply": "2024-06-03T03:41:56.054979Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q8. Top 3 best performing Supervisors**"
      ],
      "metadata": {
        "id": "4YbDiZh7A-_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Groupby Supervisor Wise w.r.t Customer satisfaction score data\n",
        "grouped_df = df_best_score.groupby('Supervisor').agg(\n",
        "    Count=('CSAT Score', 'size')\n",
        ").sort_values(by='Count',ascending=False)\n",
        "\n",
        "grouped_df[:3]"
      ],
      "metadata": {
        "id": "8SwK6JJpAo7k",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:56.05694Z",
          "iopub.execute_input": "2024-06-03T03:41:56.057233Z",
          "iopub.status.idle": "2024-06-03T03:41:56.077365Z",
          "shell.execute_reply.started": "2024-06-03T03:41:56.057208Z",
          "shell.execute_reply": "2024-06-03T03:41:56.076473Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the data\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "grouped_df[:3].plot(kind='bar', ax=ax)\n",
        "\n",
        "# Adding labels and title\n",
        "ax.set_xlabel('Supervisor Name')\n",
        "ax.set_ylabel('Count of CSAT Scores')\n",
        "ax.set_title('Supervisors with highest Customer Satisfaction Score')\n",
        "\n",
        "# Rotating the x-axis labels for better readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Adding grid for better readability\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w-c5WHwRAzhE",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:56.087259Z",
          "iopub.execute_input": "2024-06-03T03:41:56.087586Z",
          "iopub.status.idle": "2024-06-03T03:41:56.334275Z",
          "shell.execute_reply.started": "2024-06-03T03:41:56.087563Z",
          "shell.execute_reply": "2024-06-03T03:41:56.333298Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q9. Which tenure group of employees is performing the best?**"
      ],
      "metadata": {
        "id": "EzCh2lklBEsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Groupby Supervisor Wise w.r.t Customer satisfaction score data\n",
        "grouped_df = df_best_score.groupby('Tenure Bucket').agg(\n",
        "    Count=('CSAT Score', 'size')\n",
        ").sort_values(by='Count',ascending=False)\n",
        "\n",
        "grouped_df[:3]"
      ],
      "metadata": {
        "id": "NnTFPAV8B7Yq",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:56.335641Z",
          "iopub.execute_input": "2024-06-03T03:41:56.336144Z",
          "iopub.status.idle": "2024-06-03T03:41:56.357423Z",
          "shell.execute_reply.started": "2024-06-03T03:41:56.336107Z",
          "shell.execute_reply": "2024-06-03T03:41:56.356536Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the data\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "grouped_df[:3].plot(kind='bar', ax=ax)\n",
        "\n",
        "# Adding labels and title\n",
        "ax.set_xlabel('Tenure bucket')\n",
        "ax.set_ylabel('Count of CSAT Scores')\n",
        "ax.set_title('Tenure group with highest Customer Satisfaction Score')\n",
        "\n",
        "# Rotating the x-axis labels for better readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Adding grid for better readability\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D7je_-GHCDny",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:56.358996Z",
          "iopub.execute_input": "2024-06-03T03:41:56.359685Z",
          "iopub.status.idle": "2024-06-03T03:41:56.641478Z",
          "shell.execute_reply.started": "2024-06-03T03:41:56.359633Z",
          "shell.execute_reply": "2024-06-03T03:41:56.640603Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q10. Which shift timings of agents is performing the best?**"
      ],
      "metadata": {
        "id": "kmPcFk5iFH6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Groupby Supervisor Wise w.r.t Customer satisfaction score data\n",
        "grouped_df = df_best_score.groupby('Agent Shift').agg(\n",
        "    Count=('CSAT Score', 'size')\n",
        ").sort_values(by='Count',ascending=False)\n",
        "\n",
        "grouped_df[:3]"
      ],
      "metadata": {
        "id": "8ezRFAPLE9w4",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:56.642832Z",
          "iopub.execute_input": "2024-06-03T03:41:56.643185Z",
          "iopub.status.idle": "2024-06-03T03:41:56.663925Z",
          "shell.execute_reply.started": "2024-06-03T03:41:56.643145Z",
          "shell.execute_reply": "2024-06-03T03:41:56.662979Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the data\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "grouped_df[:3].plot(kind='bar', ax=ax)\n",
        "\n",
        "# Adding labels and title\n",
        "ax.set_xlabel('Agents Shift Timings')\n",
        "ax.set_ylabel('Count of CSAT Scores')\n",
        "ax.set_title('Shift Timings with highest Customer Satisfaction Score')\n",
        "\n",
        "# Rotating the x-axis labels for better readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Adding grid for better readability\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VIROyUW4E_xx",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:56.665236Z",
          "iopub.execute_input": "2024-06-03T03:41:56.665604Z",
          "iopub.status.idle": "2024-06-03T03:41:56.960721Z",
          "shell.execute_reply.started": "2024-06-03T03:41:56.665572Z",
          "shell.execute_reply": "2024-06-03T03:41:56.959776Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q11. How response time impacts the customer satisfaction score?**"
      ],
      "metadata": {
        "id": "XYy5gcchKnaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the 'Issue reported at' and 'Issue responded' columns are in datetime format\n",
        "df['Issue_reported at'] = pd.to_datetime(df['Issue_reported at'], dayfirst=True)\n",
        "df['issue_responded'] = pd.to_datetime(df['issue_responded'], dayfirst=True)\n",
        "\n",
        "# Calculate the response time\n",
        "df['Response_Time'] = df['issue_responded'] - df['Issue_reported at']\n",
        "\n",
        "# Convert 'Response_Time' to a numerical format in seconds for aggregation\n",
        "df['Response_Time_seconds'] = df['Response_Time'].dt.total_seconds()\n",
        "\n",
        "# Groupby CSAT Score and calculate the mean response time\n",
        "grouped_df = df.groupby('CSAT Score').agg(\n",
        "    Mean_Response_Time=('Response_Time_seconds', 'mean')\n",
        ").sort_values(by='Mean_Response_Time', ascending=False)\n",
        "\n",
        "# Convert the mean response time back to timedelta for readability\n",
        "grouped_df['Mean_Response_Time'] = pd.to_timedelta(grouped_df['Mean_Response_Time'], unit='s')\n",
        "\n",
        "# Display the grouped DataFrame\n",
        "print(grouped_df)"
      ],
      "metadata": {
        "id": "hP-WWOvBG1k7",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:56.961945Z",
          "iopub.execute_input": "2024-06-03T03:41:56.962218Z",
          "iopub.status.idle": "2024-06-03T03:41:57.860188Z",
          "shell.execute_reply.started": "2024-06-03T03:41:56.962195Z",
          "shell.execute_reply": "2024-06-03T03:41:57.859203Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the data\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "grouped_df.plot(kind='bar', ax=ax)\n",
        "\n",
        "# Adding labels and title\n",
        "ax.set_xlabel('CSAT Scores')\n",
        "ax.set_ylabel('Mean Response Time')\n",
        "ax.set_title('Mean Response Time in each Customer Satisfaction Score')\n",
        "\n",
        "# Rotating the x-axis labels for better readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Adding grid for better readability\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TfKn5cIMKEcB",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:57.861476Z",
          "iopub.execute_input": "2024-06-03T03:41:57.861805Z",
          "iopub.status.idle": "2024-06-03T03:41:58.193451Z",
          "shell.execute_reply.started": "2024-06-03T03:41:57.861779Z",
          "shell.execute_reply": "2024-06-03T03:41:58.192594Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q12. How customer handling time duration impacts the customer satisfaction score?**"
      ],
      "metadata": {
        "id": "h7gi-LRXLeZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Groupby Customer satisfaction score data w.r.t customer handling time\n",
        "grouped_df = df.groupby('CSAT Score').agg(\n",
        "    Mean_Response_Time=('connected_handling_time', 'mean')\n",
        ").sort_values(by='Mean_Response_Time', ascending=False)\n",
        "\n",
        "grouped_df"
      ],
      "metadata": {
        "id": "alkDICeyLs2M",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:58.194743Z",
          "iopub.execute_input": "2024-06-03T03:41:58.195528Z",
          "iopub.status.idle": "2024-06-03T03:41:58.212539Z",
          "shell.execute_reply.started": "2024-06-03T03:41:58.195495Z",
          "shell.execute_reply": "2024-06-03T03:41:58.211673Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the data\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "grouped_df.plot(kind='bar', ax=ax)\n",
        "\n",
        "# Adding labels and title\n",
        "ax.set_xlabel('CSAT Scores')\n",
        "ax.set_ylabel('Mean Customer Handling Time')\n",
        "ax.set_title('Mean Customer Handling Time in each Customer Satisfaction Score')\n",
        "\n",
        "# Rotating the x-axis labels for better readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Adding grid for better readability\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "52fva9q2Mt7d",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:58.213519Z",
          "iopub.execute_input": "2024-06-03T03:41:58.21381Z",
          "iopub.status.idle": "2024-06-03T03:41:58.516908Z",
          "shell.execute_reply.started": "2024-06-03T03:41:58.213786Z",
          "shell.execute_reply": "2024-06-03T03:41:58.515955Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the provided data, we aimed to gain a clear understanding of customer satisfaction scores through graphical representations. However, it is crucial to delve deeper into the behavior of customers with varying satisfaction scores to uncover insights and hypothetical statements that might explain the reasons behind these scores. Thus, I focused on the data of customers with high satisfaction scores to identify patterns and potential reasons for their satisfaction.\n",
        "\n",
        "Potential reasons for lower customer satisfaction scores are noted below based on the findings from the analysis:\n",
        "\n",
        "**Insights from Analysis:**\n",
        "\n",
        "**Response Time:** Identified that longer response times were correlated with lower customer satisfaction scores. This suggests a need for quicker response mechanisms.\n",
        "\n",
        "**Product Category:** Found that certain product categories had consistently lower satisfaction scores, indicating potential issues with these products or their support processes.\n",
        "\n",
        "**Channel Name:** Discovered that certain customer service channels were more effective at resolving issues satisfactorily, leading to higher CSAT scores.\n",
        "\n",
        "**Agent Tenure:** Noted that agents with longer tenures tended to receive higher satisfaction scores, suggesting that experience plays a crucial role in customer service effectiveness.\n",
        "\n",
        "**Shift Timings:** Found variations in satisfaction scores based on agent shifts, with some shifts having lower scores, possibly due to higher workloads or fewer resources during those times.\n",
        "\n",
        "**Customer Feedback:** Analyzed customer remarks to identify common themes and keywords associated with low satisfaction scores, providing qualitative insights into customer pain points.\n",
        "\n"
      ],
      "metadata": {
        "id": "l7Sf9RblRrPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 - Pie Chart on Dependant Variable i.e., CSAT Score (Univariate)"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Dependant Column Value Counts\n",
        "# Display the value counts of the 'CSAT Score' column\n",
        "print(df['CSAT Score'].value_counts())\n",
        "print(\" \")\n",
        "\n",
        "# Visualize the 'CSAT Score' value counts as a pie chart\n",
        "df['CSAT Score'].value_counts().plot(\n",
        "    kind='pie',\n",
        "    figsize=(15, 6),\n",
        "    autopct=\"%1.1f%%\",\n",
        "    startangle=90,\n",
        "    shadow=True,\n",
        "    labels=df['CSAT Score'].value_counts().index,\n",
        "    colors=plt.cm.Paired(range(len(df['CSAT Score'].value_counts()))),\n",
        "    explode=[0.1] * len(df['CSAT Score'].value_counts())  # Slightly explode all slices for better visibility\n",
        ")\n",
        "\n",
        "# Set the title and display the plot\n",
        "plt.title('Customer Satisfaction Score Distribution')\n",
        "plt.ylabel('')  # Hide the y-label as it's redundant in a pie chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:58.517978Z",
          "iopub.execute_input": "2024-06-03T03:41:58.518231Z",
          "iopub.status.idle": "2024-06-03T03:41:58.752143Z",
          "shell.execute_reply.started": "2024-06-03T03:41:58.518209Z",
          "shell.execute_reply": "2024-06-03T03:41:58.750814Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart expresses a part-to-whole relationship in your data. It's easy to explain the percentage comparison through area covered in a circle with different colors. Where differenet percentage comparison comes into action pie chart is used frequently. So, I used Pie chart and which helped me to get the percentage comparision of the dependant variable."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the chart, I observed that 59,617 customers rated the service with a CSAT Score of 5, which accounts for 69.4% of the total feedback in the dataset. Conversely, 1,283 customers were dissatisfied and gave a CSAT Score of 2, representing 1.5% of the total responses.\n",
        "\n",
        "Additionally, 13.1% of customers gave a poor CSAT score of 1, another 13.1% rated it as 4, and 3% of customers provided a score of 3. This means nearly 15% of customers experienced poor service. Therefore, it is crucial to examine the factors contributing to this dissatisfaction.\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "DxquiKWpK14X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the gained insights will help create a positive business impact. Here's how:\n",
        "\n",
        "**Focus on Strengths:** Knowing that 69.4% of customers rated the service with a CSAT score of 5 indicates a strong positive reception. By analyzing what is working well for these satisfied customers, the business can replicate these strategies across other areas to further enhance customer satisfaction.\n",
        "\n",
        "**Targeted Improvements:** Identifying that 15% of customers are experiencing poor service (CSAT scores of 1, 2, and 3) allows the business to focus on specific areas of improvement. Understanding the reasons behind these low scores can help address the root causes, such as response time, service quality, or specific product issues.\n",
        "\n",
        "**Resource Allocation:** Insights into customer satisfaction distribution can guide the allocation of resources. For instance, more training and support can be provided to agents or departments that receive lower scores to elevate their performance.\n",
        "\n",
        "**Strategic Planning:** The data can be used to set targeted goals for improvement in customer satisfaction metrics, driving a continuous improvement culture within the organization.\n",
        "Are there any insights that lead to negative growth? Justify with specific reasons.\n",
        "\n",
        "While the insights primarily aim to create a positive impact, if not properly managed, they could potentially lead to negative growth:\n",
        "\n",
        "**Neglecting High Performers:** If the focus shifts too heavily on addressing negative feedback without recognizing and maintaining what leads to high satisfaction (69.4% with a score of 5), there is a risk of neglecting the positive aspects. This could inadvertently lead to a decline in the areas that are currently performing well.\n",
        "\n",
        "**Inadequate Response to Poor Scores:** If the business fails to adequately address the issues leading to the 15% of poor scores, customer dissatisfaction could worsen. Dissatisfied customers are more likely to churn, leave negative reviews, and dissuade potential customers, negatively impacting growth.\n",
        "Overemphasis on Quick Fixes: Prioritizing quick fixes over sustainable, long-term solutions can lead to temporary improvements in CSAT scores without addressing underlying issues. This might result in a superficial improvement in customer satisfaction but could cause long-term dissatisfaction if deeper problems are ignored.\n",
        "\n",
        "**Justification with Specific Reasons**\n",
        "\n",
        "**Positive Business Impact:** The insights provide a clear indication of customer satisfaction levels and areas needing improvement. For instance, since a significant majority (69.4%) are highly satisfied, the business can study and reinforce the strategies that contribute to high satisfaction. Additionally, addressing the 15% of poor scores by understanding and resolving their causes will likely result in improved overall customer satisfaction and loyalty.\n",
        "\n",
        "**Potential for Negative Growth:** Ignoring the insights related to low satisfaction scores or failing to act on them effectively could lead to increased dissatisfaction. For example, if the business does not address the issues faced by the 1.5% of customers who gave a score of 2, this dissatisfaction can spread, potentially leading to higher churn rates and negative word-of-mouth. Similarly, failing to balance efforts between maintaining high satisfaction levels and improving lower ones can also be detrimental.\n",
        "\n",
        "In conclusion, the insights have the potential to drive positive business impact by highlighting areas of strength and weakness. However, careful and balanced management of these insights is crucial to avoid any negative consequences and ensure sustained growth and customer satisfaction."
      ],
      "metadata": {
        "id": "hZ1DGDxyKc6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 - Agent Vs. Average Response Time Percentage (Bivariate with Categorical - Numerical)"
      ],
      "metadata": {
        "id": "xJivPyE8q_2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Showing Average True Churn Percentage state wise\n",
        "# Showing top 10 churned state\n",
        "print((df.groupby(['Agent_name'])['Response_Time_seconds'].mean()*100).sort_values(ascending = False).reset_index(name=\"Average Response Time %\").head(10))\n",
        "print(\" \")\n",
        "\n",
        "# State vs. average true churn percantage visualization code\n",
        "# Vizualizing top 10 churned state\n",
        "plt.rcParams['figure.figsize'] = (12, 7)\n",
        "color = plt.cm.copper(np.linspace(0, 0.5, 20))\n",
        "((df.groupby(['Agent_name'])['Response_Time_seconds'].mean())*100).sort_values(ascending = False).head(10).plot.bar(color = ['violet','indigo','b','g','y','orange','r'])\n",
        "plt.title(\" Agent average Response_Time_seconds percentage\", fontsize = 20)\n",
        "plt.xlabel('Agent', fontsize = 15)\n",
        "plt.ylabel('percentage', fontsize = 15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NTkxmIkWq_20",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:58.754075Z",
          "iopub.execute_input": "2024-06-03T03:41:58.755271Z",
          "iopub.status.idle": "2024-06-03T03:41:59.243443Z",
          "shell.execute_reply.started": "2024-06-03T03:41:58.755222Z",
          "shell.execute_reply": "2024-06-03T03:41:59.242469Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "azX1PEddq_20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts show the frequency counts of values for the different levels of a categorical or nominal variable. Sometimes, bar charts show other statistics, such as percentages.\n",
        "\n",
        "To show the average percentage of response time with respect to agents, I have used Bar Chart."
      ],
      "metadata": {
        "id": "QTXVnJ-fq_20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "iyKleWeyq_20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 10 agents with varying average response times.\n",
        "\n",
        "The average response times by agent range from 2.09 to 4.09 hours. Elizabeth Rose and Donald Jordan have the shortest average response times, providing the best service to their clients through prompt action.\n",
        "\n",
        "On the other hand, Christine Castro has the longest average response time for addressing client queries. Therefore, evaluating her performance and providing additional training is crucial to enhance the CSAT Score."
      ],
      "metadata": {
        "id": "DwesNe8zq_21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "8-UX51ofq_21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights gained can help create a positive business impact. By identifying which agents have the shortest and longest response times, businesses can take specific actions to improve overall customer satisfaction:\n",
        "\n",
        "**Performance Recognition:** Recognizing and rewarding agents like Elizabeth Rose and Donald Jordan for their exemplary service can boost morale and set a benchmark for other agents.\n",
        "\n",
        "**Targeted Training:** Providing additional training and support to agents like Christine Castro can help reduce response times, leading to better customer experiences and potentially higher CSAT scores.\n",
        "\n",
        "**Resource Allocation:** Understanding the distribution of response times can help in reallocating resources and support where needed most, ensuring a more balanced and efficient customer service operation.\n",
        "\n",
        "**Process Improvements:** Identifying bottlenecks and inefficiencies in the service process can lead to improvements that benefit all agents and customers, enhancing overall service quality.\n",
        "\n",
        "**Are there any insights that lead to negative growth?**\n",
        "\n",
        "There are no direct insights that would lead to negative growth; however, if not acted upon appropriately, some insights could potentially have a negative impact:\n",
        "\n",
        "**Failure to Address Poor Performance:** If agents with high response times are not given the necessary training and support, customer dissatisfaction may continue or worsen, leading to negative reviews and loss of customers.\n",
        "\n",
        "**Ignoring Top Performers:** Not recognizing and rewarding top-performing agents could lead to decreased motivation and performance over time, potentially affecting overall service quality.\n",
        "\n",
        "**Overemphasis on Speed:** Focusing solely on reducing response times without maintaining quality of service might lead to rushed interactions and unresolved issues, which could harm customer satisfaction in the long run.\n",
        "\n",
        "**Justification with Specific Reasons**\n",
        "\n",
        "**Positive Business Impact:** By addressing the variations in response times, the business can ensure a more consistent and satisfactory customer experience. For instance, agents like Elizabeth Rose and Donald Jordan, who provide quick responses, set a standard for others. This can be leveraged through training programs to improve the performance of other agents.\n",
        "\n",
        "**Potential for Negative Growth:** If insights are ignored, such as the need for retraining agents with higher response times like Christine Castro, customer dissatisfaction may persist. Dissatisfied customers are more likely to churn and spread negative word-of-mouth, which can harm the businesss reputation and growth prospects.\n",
        "\n",
        "In conclusion, the insights gained will likely foster a positive business impact if acted upon effectively, leading to improved customer satisfaction and service quality. However, neglecting these insights or mismanaging the response to them could result in negative growth."
      ],
      "metadata": {
        "id": "nW7-bqD0q_21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 - Box Plot on Connected handling time with CSAT Score (Bivariate)"
      ],
      "metadata": {
        "id": "Of3PJYNbrGff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Box Plot for connected_handling_time attribute w.r.t to CSAT Score\n",
        "df.boxplot(column='connected_handling_time',by='CSAT Score')"
      ],
      "metadata": {
        "id": "o8OOv8dKphvC",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:59.244498Z",
          "iopub.execute_input": "2024-06-03T03:41:59.244813Z",
          "iopub.status.idle": "2024-06-03T03:41:59.577783Z",
          "shell.execute_reply.started": "2024-06-03T03:41:59.244776Z",
          "shell.execute_reply": "2024-06-03T03:41:59.576789Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "w8bcvZxarGfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Box plots are used to show distributions of numeric data values, especially when you want to compare them between multiple groups. They are built to provide high-level information at a glance, offering general information about a group of data's symmetry, skew, variance, and outliers. So, I used box plot to get the maximum and minimum value with well sagreggated outliers with well defined mean and median as shown in the box plot graph."
      ],
      "metadata": {
        "id": "9-GBsMEqrGfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "HZ6txCBBrGfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above boxplot, we can observe that there are a few outliers in the CSAT Scores of 4 and 5. Specifically, outliers appear when the connected handling time exceeds 750 for CSAT Score 4, and when it exceeds 1000 for CSAT Score 5. Analyzing these outliers is crucial for understanding the underlying factors contributing to these anomalies.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9X7Ff9HFrGfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "jbkhcZqdrGfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights gained can help create a positive business impact. Here's how:\n",
        "\n",
        "**Targeted Improvements:** By identifying outliers in CSAT Scores when the connected handling time is high, the business can focus on improving processes that lead to long handling times. Reducing these handling times can enhance customer satisfaction.\n",
        "**Quality Control:** Understanding why CSAT Scores drop when handling times increase will allow the business to implement quality control measures. This can involve additional training for agents, better resource allocation, or process optimizations.\n",
        "**Customer Experience Enhancement:** By addressing the factors leading to long handling times and subsequent lower satisfaction scores, the business can improve the overall customer experience, which can lead to increased loyalty and positive word-of-mouth.\n",
        "\n",
        "Are there any insights that lead to negative growth?\n",
        "\n",
        "While the primary goal of the insights is to foster positive business impact, there could be potential risks if not managed properly:\n",
        "\n",
        "**Overemphasis on Speed:** If the business focuses too much on reducing handling times without ensuring the quality of interactions, it might lead to rushed and ineffective customer service. This can result in unresolved issues and lower overall satisfaction.\n",
        "\n",
        "**Neglecting Non-Outlier Data:** Focusing exclusively on outliers might lead to neglecting the broader dataset. Improvements should be holistic, ensuring that all areas of customer service are enhanced, not just those with extreme values.\n",
        "\n",
        "\n",
        "**Justification with Specific Reasons**\n",
        "\n",
        "**Positive Business Impact:** Addressing the outliers in CSAT Scores related to high handling times can directly improve customer satisfaction by ensuring quicker and more efficient service. For instance, by training agents to handle calls more effectively or by implementing better call routing systems, the business can reduce handling times and thus improve scores.\n",
        "\n",
        "**Potential for Negative Growth:** If the business focuses solely on reducing handling times without maintaining the quality of interactions, it may lead to superficial improvements in satisfaction scores. For example, customers might experience quicker service but still be dissatisfied if their issues are not fully resolved. Additionally, neglecting other areas in need of improvement can result in an overall decline in service quality.\n",
        "\n",
        "In conclusion, while the insights can lead to positive business impacts by targeting and resolving specific issues, a balanced and comprehensive approach is essential to avoid any potential negative growth and ensure sustainable improvements in customer satisfaction."
      ],
      "metadata": {
        "id": "sZjMyjAHrGfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 - CSAT Score vs Item price (Bivariate)"
      ],
      "metadata": {
        "id": "riLp7y9brHca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# CSAT Score wise average Item_price Percentage\n",
        "# Calculate the average item price percentage by CSAT Score\n",
        "csat_avg_item_price_percentage = dataset.groupby('CSAT Score')['Item_price'].mean() * 100\n",
        "print(csat_avg_item_price_percentage)\n",
        "print(\" \")\n",
        "\n",
        "# Visualizing the CSAT Score wise average item price percentage\n",
        "plt.bar(csat_avg_item_price_percentage.index, csat_avg_item_price_percentage, color=['r', 'b', 'g', 'c', 'm'])\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (10, 6)  # Adjust the figure size\n",
        "plt.xlabel('CSAT Score', fontsize=15)\n",
        "plt.ylabel('Item Price Percentage', fontsize=15)\n",
        "plt.title('CSAT Score Wise Average Item Price Percentage', fontsize=18)\n",
        "plt.xticks(csat_avg_item_price_percentage.index, fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QQ-DWHW5rHca",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:59.579302Z",
          "iopub.execute_input": "2024-06-03T03:41:59.579735Z",
          "iopub.status.idle": "2024-06-03T03:41:59.873777Z",
          "shell.execute_reply.started": "2024-06-03T03:41:59.579699Z",
          "shell.execute_reply": "2024-06-03T03:41:59.872777Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "Uw0GT_uHrHcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts show the frequency counts of values for the different levels of a categorical or nominal variable. Sometimes, bar charts show other statistics, such as percentages.\n",
        "\n",
        "To show the average percentage of true churn with respect to Area Code, I have used Bar Chart."
      ],
      "metadata": {
        "id": "R7c_a-IXrHcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "qTEiGUKorHcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the bar graph, it is evident that the mean item price is highest when the CSAT score is 1 and lowest when the CSAT score is 5. This indicates an inverse correlation between item price and CSAT score, suggesting that higher item prices are generally associated with lower customer satisfaction scores.\n"
      ],
      "metadata": {
        "id": "a3rmtW3vrHcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "1Qw8mtL3rHcd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights can help create a positive business impact in the following ways:\n",
        "\n",
        "**Pricing Strategy:** Understanding that higher item prices are associated with lower CSAT scores can guide pricing strategies. By adjusting prices or offering better value at higher price points, businesses can improve customer satisfaction.\n",
        "\n",
        "**Customer Segmentation:** These insights can help in segmenting customers based on their spending behavior and satisfaction levels. Targeted promotions and personalized offers can be designed to enhance satisfaction among different customer segments.\n",
        "\n",
        " **Are there any insights that lead to negative growth?**\n",
        "\n",
        "There are potential risks if the insights are not managed properly:\n",
        "\n",
        "**Price Reduction Risks:** Simply lowering prices to improve CSAT scores might not be sustainable and could negatively impact profitability. Businesses need to balance price adjustments with maintaining profit margins.\n",
        "\n",
        "**Overemphasis on Price:** Focusing solely on price without addressing other factors that contribute to customer satisfaction (such as product quality, customer service, and overall experience) may not yield the desired improvement in CSAT scores.\n",
        "\n",
        "**Justification with Specific Reasons**\n",
        "\n",
        "**Positive Business Impact:** By aligning pricing strategies with customer expectations, businesses can enhance customer satisfaction. For instance, offering more features or better service for higher-priced items can justify the cost and improve CSAT scores. Additionally, personalized marketing strategies based on customer segments can lead to increased loyalty and repeat purchases.\n",
        "\n",
        "**Potential for Negative Growth:** If businesses reduce prices without maintaining value, it can lead to a perception of reduced quality. For example, if a high-end product's price is reduced significantly without adding corresponding value, customers might perceive it as less premium, leading to lower sales. Additionally, overemphasizing price reductions can erode profit margins, affecting the overall financial health of the business.\n",
        "\n",
        "In conclusion, while the insights provide valuable guidance for improving customer satisfaction and driving positive business impact, it is crucial to implement them thoughtfully. Balancing price adjustments with value enhancement and considering all factors affecting customer satisfaction will help avoid potential negative consequences and ensure sustainable growth.\n",
        "\n"
      ],
      "metadata": {
        "id": "t7dguLynrHcd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5- Column wise Histogram & Box Plot Univariate Analysis"
      ],
      "metadata": {
        "id": "I3aNiIh9fKqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Visualizing code of hist plot for each columns to know the data distibution\n",
        "for col in dataset.describe().columns:\n",
        "  fig=plt.figure(figsize=(9,6))\n",
        "  ax=fig.gca()\n",
        "  feature= (dataset[col])\n",
        "  sns.distplot(dataset[col])\n",
        "  ax.axvline(feature.mean(),color='magenta', linestyle='dashed', linewidth=2)\n",
        "  ax.axvline(feature.median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "  ax.set_title(col)\n",
        "plt.show()\n",
        "\n",
        "# Visualizing code of box plot for each columns to know the data distibution\n",
        "for col in dataset.describe().columns:\n",
        "    fig = plt.figure(figsize=(9, 6))\n",
        "    ax = fig.gca()\n",
        "    dataset.boxplot( col, ax = ax)\n",
        "    ax.set_title('BoxPlot Label by ' + col)\n",
        "    #ax.set_ylabel(\"Churn\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WPVINT_UfKqa",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:41:59.874871Z",
          "iopub.execute_input": "2024-06-03T03:41:59.875125Z",
          "iopub.status.idle": "2024-06-03T03:42:02.378961Z",
          "shell.execute_reply.started": "2024-06-03T03:41:59.875104Z",
          "shell.execute_reply": "2024-06-03T03:42:02.377921Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "hTF1e9PwfKqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram is a popular graphing tool. It is used to summarize discrete or continuous data that are measured on an interval scale. It is often used to illustrate the major features of the distribution of the data in a convenient form. It is also useful when dealing with large data sets (greater than 100 observations). It can help detect any unusual observations (outliers) or any gaps in the data.\n",
        "\n",
        "Thus, I used the histogram plot to analysis the variable distributions over the whole dataset whether it's symmetric or not.\n",
        "\n",
        "Box plots are used to show distributions of numeric data values, especially when you want to compare them between multiple groups. They are built to provide high-level information at a glance, offering general information about a group of data's symmetry, skew, variance, and outliers.\n",
        "\n",
        "Thus, for each numerical varibale in the given dataset, I used box plot to analyse the outliers and interquartile range including mean, median, maximum and minimum value."
      ],
      "metadata": {
        "id": "XOYaZpKpfKqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "1xrBLU6IfKqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Connected Handling Time\" feature is symmetrically distributed, with the mean being almost the same as the median for numerical columns. However, the \"Item Price\" feature does not follow a symmetric distribution and contains noise.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rrwjuBlHfKqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "yiLHMWFSfKqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just a histogram and box plot cannot define business impact. It's done just to see the distribution of the column data over the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "7cTtRDwufKqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation matrix\n",
        "correlation_matrix = df[df.describe().columns.to_list()].corr()\n",
        "\n",
        "# Select only the correlation of the target variable with other features\n",
        "target_variable='CSAT Score'\n",
        "correlation_with_target = correlation_matrix[[target_variable]].sort_values(by=target_variable, ascending=False)\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_with_target, annot=True, cmap='coolwarm', cbar=True)\n",
        "plt.title(f'Correlation of {target_variable} with Independent Numerical Features')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nKTPkZZfwmCE",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:02.380404Z",
          "iopub.execute_input": "2024-06-03T03:42:02.380782Z",
          "iopub.status.idle": "2024-06-03T03:42:02.809687Z",
          "shell.execute_reply.started": "2024-06-03T03:42:02.380749Z",
          "shell.execute_reply": "2024-06-03T03:42:02.808719Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses. The range of correlation is [-1,1].\n",
        "\n",
        "Thus to know the correlation between all the variables along with the correlation coeficients, i used correlation heatmap."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above correlation heatmap, we can see that \"Issue Reported,\" \"Issue Responded,\" and \"Connected Handling Time\" are moderately positively correlated with the CSAT Score.\n",
        "\n",
        "Additionally, \"Connected Handling Time\" has a positive correlation with the CSAT Score and a negative correlation with both \"Response Time\" and \"Item Price.\"\n",
        "\n",
        "All other correlations can be observed from the chart.\n"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(df, hue=\"CSAT Score\")"
      ],
      "metadata": {
        "id": "o58-TEIhveiU",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:02.811757Z",
          "iopub.execute_input": "2024-06-03T03:42:02.812128Z",
          "iopub.status.idle": "2024-06-03T03:42:12.052465Z",
          "shell.execute_reply.started": "2024-06-03T03:42:02.812095Z",
          "shell.execute_reply": "2024-06-03T03:42:12.051561Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pair plot is used to understand the best set of features to explain a relationship between two variables or to form the most separated clusters. It also helps to form some simple classification models by drawing some simple lines or make linear separation in our data-set.\n",
        "\n",
        "Thus, I used pair plot to analyse the patterns of data and realationship between the features. It's exactly same as the correlation map but here you will get the graphical representation."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart I got to know, there are less linear relationship between variables and data points aren't linearly separable. Customers feedback data is clusetered and ovearlapped each other. connected_handling_time are quite symmetrical in nature and item_price feature and response time are quite non symmetric in nature. In this whole pair plot, the importance of response time can be seen and the connected_hanling_time with respect to different features are really insightful. Rest insights can be depicted from the above graph."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **When the Mean Response Time is less than 2, the Customer Satisfaction Score is 5.**\n",
        "2. **When the price of an item above 5660, does it result in customer satisfaction scores to go below 3**\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hypothetical Statement - 1**\n",
        "**When the Mean Response Time is less than 2, the Customer Satisfaction Score is 5.**"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): The mean Response Time is equal to 2 when the CSAT Score is 5.\n",
        "\n",
        "Alternative Hypothesis (H1): The mean Response Time is less than 2 when the CSAT Score is 5.\n",
        "\n",
        "Perform One-Sample t-test:\n",
        "\n",
        "We will use a one-sample t-test to compare the sample mean of Response Time against the population mean (2).\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import ttest_1samp\n",
        "\n",
        "\n",
        "\n",
        "# Step 1: Filter the data for CSAT Score of 5\n",
        "df_csat_5 = df[df['CSAT Score'] == 5]\n",
        "\n",
        "# Step 2: Calculate the mean Response Time\n",
        "mean_response_time = df_csat_5['Response_Time_seconds'].mean()\n",
        "\n",
        "# Step 3: Perform one-sample t-test\n",
        "# Null Hypothesis: Mean Response Time = 2*3600 (2 hours converted to seconds)\n",
        "hypothesized_mean = 2 * 3600  # 2 hours in seconds\n",
        "\n",
        "# Perform the t-test\n",
        "t_stat, p_value = ttest_1samp(df_csat_5['Response_Time_seconds'], hypothesized_mean)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Mean Response Time: {mean_response_time} seconds\")\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "# Step 4: Conclusion\n",
        "alpha = 0.05  # significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the Null Hypothesis: The mean Response Time is significantly less than 2 hours when the CSAT Score is 5.\")\n",
        "else:\n",
        "    print(\"Fail to Reject the Null Hypothesis: There is no significant evidence that the mean Response Time is less than 2 hours when the CSAT Score is 5.\")\n"
      ],
      "metadata": {
        "id": "VHLqHrvJIsWp",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:12.053527Z",
          "iopub.execute_input": "2024-06-03T03:42:12.053825Z",
          "iopub.status.idle": "2024-06-03T03:42:12.082932Z",
          "shell.execute_reply.started": "2024-06-03T03:42:12.053801Z",
          "shell.execute_reply": "2024-06-03T03:42:12.081996Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used t-Test as the statistical testing to obtain P-Value and found the result that Null hypothesis has been rejected.\n",
        "\n",
        "Based on the results of the one-sample t-test, the following findings can be made:\n",
        "\n",
        "**Mean Response Time:**\n",
        "\n",
        "The mean response time for customers who gave a CSAT Score of 5 is approximately 5706.44 seconds (about 1.58 hours).\n",
        "\n",
        "**T-statistic and P-value:**\n",
        "\n",
        "The t-statistic is -11.85, indicating that the observed mean response time is significantly different from the hypothesized mean of 7200 seconds (2 hours).\n",
        "The p-value is extremely small (2.30e-32), which is far below the significance level of 0.05.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "Given the p-value is much less than the significance level of 0.05, we reject the null hypothesis.\n",
        "\n",
        "This means there is strong statistical evidence to conclude that the mean response time for customers who rated the service with a CSAT Score of 5 is significantly less than 2 hours.\n",
        "\n",
        "**Business Implication:**\n",
        "\n",
        "The significantly lower response time for customers with a high satisfaction score suggests that prompt response times are correlated with higher customer satisfaction.\n",
        "\n",
        "Focusing on reducing response times could be a key strategy to enhance overall customer satisfaction.\n",
        "\n",
        "This analysis indicates that improving response times can positively impact customer satisfaction scores, supporting efforts to maintain or enhance quick response rates in customer service operations."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rationale for Choosing the One-Sample T-Test:**\n",
        "\n",
        "**Nature of the Data:**\n",
        "\n",
        "We have a single sample of response times for customers who gave a CSAT Score of 5.\n",
        "\n",
        "We need to compare the mean of this sample to a known value (2 hours or 7200 seconds).\n",
        "\n",
        "**Continuous Variable:**\n",
        "\n",
        "Response time is a continuous variable measured in seconds.\n",
        "The t-test is suitable for comparing means of continuous data.\n",
        "\n",
        "**Comparing to a Hypothesized Value:**\n",
        "\n",
        "The one-sample t-test is designed to determine whether the sample mean is significantly different from a known or hypothesized population mean.\n",
        "In this case, we are comparing the mean response time to the hypothesized value of 7200 seconds (2 hours).\n",
        "\n",
        "**Small Sample Size or Unknown Population Variance:**\n",
        "\n",
        "If the population variance is unknown and the sample size is reasonably small, the t-test is appropriate as it accounts for sample size in its calculation.\n",
        "The t-distribution is used instead of the normal distribution when the sample size is small or the population variance is unknown.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "The one-sample t-test was chosen because it effectively tests whether the mean response time for a sample (customers who rated the service with a CSAT Score of 5) is significantly different from a specified value (2 hours). The test provides a t-statistic and p-value that help determine if the observed difference is statistically significant, thereby allowing us to make an informed conclusion regarding the hypothesis.\n",
        "\n",
        "This choice of test aligns with the objective of assessing the mean response time against a benchmark, making it a suitable and robust statistical method for this analysis."
      ],
      "metadata": {
        "id": "pFFfX4KQKcNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualizing the distribution of Response Time\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Histogram for Response Time\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(df['Response_Time_seconds'], bins=30, color='blue', edgecolor='black')\n",
        "plt.title('Distribution of Response Time')\n",
        "plt.xlabel('Response Time (seconds)')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Histogram for CSAT Score when it is 5\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(df[df['CSAT Score'] == 5]['Response_Time_seconds'], bins=30, color='green', edgecolor='black')\n",
        "plt.title('Distribution of Response Time (CSAT Score = 5)')\n",
        "plt.xlabel('Response Time (seconds)')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Tz1TkC-_ZdhI",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:12.084053Z",
          "iopub.execute_input": "2024-06-03T03:42:12.084337Z",
          "iopub.status.idle": "2024-06-03T03:42:12.638703Z",
          "shell.execute_reply.started": "2024-06-03T03:42:12.084313Z",
          "shell.execute_reply": "2024-06-03T03:42:12.63768Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hypothetical Statement - 2**\n",
        "**When the price of an item above 5660, does it result in customer satisfaction scores to go below 3**"
      ],
      "metadata": {
        "id": "jLrtcv83OEmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "5djnGilGOEmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis (H0):** The mean CSAT score for items priced above 5660 is not significantly different from 3.\n",
        "\n",
        "**Alternative Hypothesis (H1):** The mean CSAT score for items priced above 5660 is significantly less than 3.\n",
        "\n",
        " **Test Type :** Use a one-sample t-test to compare the mean CSAT score of the filtered data to the value 3.\n",
        "\n"
      ],
      "metadata": {
        "id": "ps8ZluDUOEmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "DADo_qtwOEmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import ttest_1samp\n",
        "\n",
        "\n",
        "\n",
        "# Step 1: Filter the Data\n",
        "high_price_df = df[df['Item_price'] > 5660]\n",
        "\n",
        "# Step 2: Perform a One-Sample t-test\n",
        "# Null Hypothesis: Mean CSAT score is 3\n",
        "# Alternative Hypothesis: Mean CSAT score is less than 3\n",
        "t_stat, p_value = ttest_1samp(high_price_df['CSAT Score'], 3)\n",
        "\n",
        "# Since it's a one-tailed test, we need to divide the p-value by 2\n",
        "p_value /= 2\n",
        "\n",
        "# Check if we reject the null hypothesis\n",
        "significance_level = 0.05\n",
        "reject_null = p_value < significance_level and t_stat < 0\n",
        "\n",
        "# Print the results\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "if reject_null:\n",
        "    print(\"Reject the Null Hypothesis: The mean CSAT score for items priced above 5660 is significantly less than 3.\")\n",
        "else:\n",
        "    print(\"Fail to Reject the Null Hypothesis: There is no significant evidence that the mean CSAT score for items priced above 5660 is less than 3.\")\n"
      ],
      "metadata": {
        "id": "pNgHgcOtN7bR",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:12.64031Z",
          "iopub.execute_input": "2024-06-03T03:42:12.640755Z",
          "iopub.status.idle": "2024-06-03T03:42:12.655745Z",
          "shell.execute_reply.started": "2024-06-03T03:42:12.640718Z",
          "shell.execute_reply": "2024-06-03T03:42:12.654805Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Findings Interpretation**\n",
        "\n",
        "Fail to Reject Null Hypothesis: The p-value is greater than 0.05, it means there is not enough evidence to suggest that items priced above 5660 significantly affect customer satisfaction scores to be below 3."
      ],
      "metadata": {
        "id": "NE2e92OSP2y_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of Output**\n",
        "\n",
        "**T-statistic:** This value indicates how many standard deviations the sample mean is away from the hypothesized mean. A negative t-statistic would support the alternative hypothesis that the sample mean is less than the hypothesized mean.\n",
        "\n",
        "**P-value:** This value tells us the probability of obtaining a result at least as extreme as the one observed, assuming the null hypothesis is true. Since we are performing a one-tailed test, the p-value is divided by 2.\n",
        "\n",
        "**Decision Rule:** If the p-value is less than the significance level (0.05) and the t-statistic is negative, we reject the null hypothesis, indicating that the mean CSAT score for high-priced items is significantly less than 3."
      ],
      "metadata": {
        "id": "xF2oLzSdPqYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "0VL2QIwzOEmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine whether the price of an item above 5660 results in customer satisfaction scores below 3, I performed a one-sample t-test. Here's a detailed explanation of the choice and procedure for the test:\n",
        "\n"
      ],
      "metadata": {
        "id": "9EZtE-psOEmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "69qjvWOxOEmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Objective:** We want to compare the mean CSAT score of a subset of data (items priced above 5660) to a specific value (3).\n",
        "\n",
        "**Type of Test:** A one-sample t-test is appropriate when you are comparing the mean of a single sample to a known or hypothesized population mean.\n",
        "\n",
        "**Assumption:** The t-test assumes that the data is approximately normally distributed, which is a reasonable assumption for many real-world data sets, especially when the sample size is large."
      ],
      "metadata": {
        "id": "7MNg8wJ1Pbkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Histogram of CSAT Scores for high priced items\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(high_price_df['CSAT Score'], kde=True, bins=10, color='skyblue')\n",
        "plt.axvline(x=3, color='red', linestyle='--')\n",
        "plt.title('Distribution of CSAT Scores for Items Priced Above 5660')\n",
        "plt.xlabel('CSAT Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend(['Hypothesized Mean (3)', 'CSAT Scores'])\n",
        "\n",
        "# Boxplot of CSAT Scores for high priced items\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(y=high_price_df['CSAT Score'], color='skyblue')\n",
        "plt.axhline(y=3, color='red', linestyle='--')\n",
        "plt.title('Boxplot of CSAT Scores for Items Priced Above 5660')\n",
        "plt.ylabel('CSAT Score')\n",
        "plt.legend(['Hypothesized Mean (3)', 'CSAT Scores'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xH-r1x7xQd75",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:12.65702Z",
          "iopub.execute_input": "2024-06-03T03:42:12.65784Z",
          "iopub.status.idle": "2024-06-03T03:42:13.355202Z",
          "shell.execute_reply.started": "2024-06-03T03:42:12.657807Z",
          "shell.execute_reply": "2024-06-03T03:42:13.354218Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram: We create a histogram to visualize the distribution of CSAT scores for items priced above 5660. The hypothesized mean (3) is marked with a red dashed line.\n",
        "\n",
        "Boxplot: We create a boxplot to visualize the spread and central tendency of the CSAT scores. The hypothesized mean (3) is again marked with a red dashed line.\n",
        "\n",
        "These visualizations help to see how the CSAT scores are distributed around the hypothesized mean and can provide a visual confirmation of the results of the hypothesis test."
      ],
      "metadata": {
        "id": "ORCHykFwQvmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a copy of the dataset for further feature engineering\n",
        "df_new=dataset.copy()"
      ],
      "metadata": {
        "id": "nEQ5IKIFtrok",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:13.3564Z",
          "iopub.execute_input": "2024-06-03T03:42:13.356721Z",
          "iopub.status.idle": "2024-06-03T03:42:13.372719Z",
          "shell.execute_reply.started": "2024-06-03T03:42:13.356691Z",
          "shell.execute_reply": "2024-06-03T03:42:13.371967Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Missing Values/Null Values Count\n",
        "print(df_new.isnull().sum())\n",
        "\n",
        "# Visualizing the missing values\n",
        "\n",
        "# Step 1: Calculate the count of missing values in each column and sort in descending order\n",
        "missing_values = df_new.isnull().sum().sort_values(ascending=False)\n",
        "\n",
        "\n",
        "# Step 2: Create a horizontal bar plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(x=missing_values, y=missing_values.index, orient='h')\n",
        "plt.xlabel('Count of Missing Values')\n",
        "plt.ylabel('Columns')\n",
        "plt.title('Count of Missing Values in Each Column')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:13.373755Z",
          "iopub.execute_input": "2024-06-03T03:42:13.374041Z",
          "iopub.status.idle": "2024-06-03T03:42:14.087305Z",
          "shell.execute_reply.started": "2024-06-03T03:42:13.374018Z",
          "shell.execute_reply": "2024-06-03T03:42:14.086412Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We employed various missing value imputation techniques based on the nature of the features and the distribution of the data:\n",
        "\n",
        "**Order_id:** As this feature is not significant for our analysis and the number of missing values is minimal, we opted to drop this column entirely.\n",
        "\n",
        "**Customer Remarks:** With a substantial number of missing values (57165), we couldn't discard this feature as it holds crucial information. Instead, we replaced the NaN values with \"Missing Reviews\" to ensure we retain the textual data for analysis.\n",
        "\n",
        "**Categorical Column Imputation (Customer city and Product Category):** Since these categorical features are vital for our analysis, we used mode imputation to fill in the missing values. Mode imputation was chosen as it replaces missing values with the most frequently occurring category, thereby preserving the distribution of the data.\n",
        "\n",
        "**Numerical Column Imputation (connected_handling_time and item_price):** For connected_handling_time, which follows a normal distribution with minimal outliers, we applied mean imputation to replace missing values. Conversely, for item_price, where outliers are more prominent, median imputation was utilized to ensure robustness against outliers.\n",
        "\n",
        "**order_date_time:** Mode imputation was applied to handle missing values in this feature, as it represents datetime data. Subsequently, we converted it into datetime format to extract additional temporal features like day and month.\n",
        "\n",
        "These techniques were selected to effectively manage missing data while preserving the integrity and utility of the dataset for subsequent analysis."
      ],
      "metadata": {
        "id": "_oD5qHxDg2Es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Drop 'Order_id' column\n",
        "df_new.drop(columns=['Order_id'], inplace=True)\n",
        "\n",
        "# Step 2: Replace missing values in 'Customer Remarks' with 'Missing Reviews'\n",
        "df_new['Customer Remarks'].fillna('Missing Reviews', inplace=True)\n",
        "\n",
        "# Step 3: Impute missing values in categorical columns ('Customer city' and 'Product Category') with mode\n",
        "df_new['Customer_City'].fillna(df_new['Customer_City'].mode()[0], inplace=True)\n",
        "df_new['Product_category'].fillna(df_new['Product_category'].mode()[0], inplace=True)\n",
        "\n",
        "# Step 4: Impute missing values in numerical columns ('connected_handling_time' and 'item_price')\n",
        "# Impute 'connected_handling_time' with mean\n",
        "df_new['connected_handling_time'].fillna(df_new['connected_handling_time'].mean(), inplace=True)\n",
        "# Impute 'item_price' with median\n",
        "df_new['Item_price'].fillna(df_new['Item_price'].median(), inplace=True)\n",
        "\n",
        "# Step 5: Impute missing values in 'order_date_time' with mode\n",
        "df_new['order_date_time'].fillna(df_new['order_date_time'].mode()[0], inplace=True)\n",
        "\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify changes\n",
        "print(df_new.head())"
      ],
      "metadata": {
        "id": "DGccVwPdhKob",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:14.088692Z",
          "iopub.execute_input": "2024-06-03T03:42:14.089147Z",
          "iopub.status.idle": "2024-06-03T03:42:14.176162Z",
          "shell.execute_reply.started": "2024-06-03T03:42:14.089108Z",
          "shell.execute_reply": "2024-06-03T03:42:14.175227Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Missing Values/Null Values Count\n",
        "print(df_new.isnull().sum())\n",
        "\n",
        "# Visualizing the missing values\n",
        "\n",
        "# Step 1: Calculate the count of missing values in each column and sort in descending order\n",
        "missing_values = df_new.isnull().sum().sort_values(ascending=False)\n"
      ],
      "metadata": {
        "id": "ImHBtox9iLzP",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:14.177485Z",
          "iopub.execute_input": "2024-06-03T03:42:14.177926Z",
          "iopub.status.idle": "2024-06-03T03:42:14.438045Z",
          "shell.execute_reply.started": "2024-06-03T03:42:14.177894Z",
          "shell.execute_reply": "2024-06-03T03:42:14.437226Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# To separate the symmetric distributed features and skew symmetric distributed features\n",
        "df_new[\"CSAT Score\"]=df_new[\"CSAT Score\"].astype('str')\n",
        "symmetric_feature=[]\n",
        "non_symmetric_feature=[]\n",
        "for i in df_new.describe().columns:\n",
        "  if abs(df_new[i].mean()-df_new[i].median())<0.2:\n",
        "    symmetric_feature.append(i)\n",
        "  else:\n",
        "    non_symmetric_feature.append(i)\n",
        "\n",
        "# Getting Symmetric Distributed Features\n",
        "print(\"Symmetric Distributed Features : -\",symmetric_feature)\n",
        "\n",
        "# Getting Skew Symmetric Distributed Features\n",
        "print(\"Skew Symmetric Distributed Features : -\",non_symmetric_feature)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:14.439179Z",
          "iopub.execute_input": "2024-06-03T03:42:14.439464Z",
          "iopub.status.idle": "2024-06-03T03:42:14.494937Z",
          "shell.execute_reply.started": "2024-06-03T03:42:14.439439Z",
          "shell.execute_reply": "2024-06-03T03:42:14.49407Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Skew Symmetric features defining upper and lower boundry\n",
        "def outlier_treatment(df,feature):\n",
        "  upper_boundary= df[feature].mean()+3*df[feature].std()\n",
        "  lower_boundary= df[feature].mean()-3*df[feature].std()\n",
        "  return upper_boundary,lower_boundary"
      ],
      "metadata": {
        "id": "VqJ-nKe-7sH1",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:14.496007Z",
          "iopub.execute_input": "2024-06-03T03:42:14.496271Z",
          "iopub.status.idle": "2024-06-03T03:42:14.501608Z",
          "shell.execute_reply.started": "2024-06-03T03:42:14.496249Z",
          "shell.execute_reply": "2024-06-03T03:42:14.50057Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restricting the data to lower and upper boundry\n",
        "for feature in non_symmetric_feature:\n",
        "  df_new.loc[df_new[feature]<= outlier_treatment(df=df_new,feature=feature)[1], feature]=outlier_treatment(df=df_new,feature=feature)[1]\n",
        "  df_new.loc[df_new[feature]>= outlier_treatment(df=df_new,feature=feature)[0], feature]=outlier_treatment(df=df_new,feature=feature)[0]"
      ],
      "metadata": {
        "id": "iu4N8d5i8Jdr",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:14.502918Z",
          "iopub.execute_input": "2024-06-03T03:42:14.503778Z",
          "iopub.status.idle": "2024-06-03T03:42:14.518615Z",
          "shell.execute_reply.started": "2024-06-03T03:42:14.503751Z",
          "shell.execute_reply": "2024-06-03T03:42:14.517658Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After Outlier Treatment showing the dataset distribution using strip plot\n",
        "# Visualising  code for the numerical columns\n",
        "for col in df_new.describe().columns:\n",
        "  fig=plt.figure(figsize=(9,6))\n",
        "  sns.stripplot(df_new[col])"
      ],
      "metadata": {
        "id": "56KLPEcOw-MQ",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:14.520181Z",
          "iopub.execute_input": "2024-06-03T03:42:14.520438Z",
          "iopub.status.idle": "2024-06-03T03:42:15.840847Z",
          "shell.execute_reply.started": "2024-06-03T03:42:14.520417Z",
          "shell.execute_reply": "2024-06-03T03:42:15.839905Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First I changed the CSAT Score column to sring as it shouldn't be treated as numerical column as there are only five type of values and should be treated as categorical column. Then I separated the skew symmetric and symmetric features and define the upper and lower boundry as defined below. Again, as it is a classification problem I restrict the both boundaries and I pull down the higher value restricted to the upper limit\n",
        "\n"
      ],
      "metadata": {
        "id": "MUP21tsOyJjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a Gaussian distribution while its the symmetric curve and outlier are present. Then, we can set the boundary by taking standard deviation into action."
      ],
      "metadata": {
        "id": "fDVHj6krj3xX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The box plot is a useful graphical display for describing the behavior of the data in the middle as well as at the ends of the distributions. The box plot uses the median and the lower and upper quartiles (defined as the 25th and 75th percentiles). If the lower quartile is Q1 and the upper quartile is Q3, then the difference (Q3  Q1) is called the interquartile range or IQ. A box plot is constructed by drawing a box between the upper and lower quartiles with a solid line drawn across the box to locate the median. The following quantities (called fences) are needed for identifying extreme values in the tails of the distribution:\n",
        "1.\tlower inner fence: Q11.5*IQ\n",
        "2.\tupper inner fence: Q3 + 1.5*IQ\n",
        "3.\tlower outer fence: Q13*IQ\n",
        "4.\tupper outer fence: Q3 + 3*IQ\n"
      ],
      "metadata": {
        "id": "LdEiDCi_kAFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.info()"
      ],
      "metadata": {
        "id": "pkGOh3drqrDc",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:15.841936Z",
          "iopub.execute_input": "2024-06-03T03:42:15.842197Z",
          "iopub.status.idle": "2024-06-03T03:42:15.987255Z",
          "shell.execute_reply.started": "2024-06-03T03:42:15.842175Z",
          "shell.execute_reply": "2024-06-03T03:42:15.986141Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.drop(columns='Unique id', inplace=True)\n"
      ],
      "metadata": {
        "id": "tYUDjg9AqAYb",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:15.988344Z",
          "iopub.execute_input": "2024-06-03T03:42:15.988623Z",
          "iopub.status.idle": "2024-06-03T03:42:16.015361Z",
          "shell.execute_reply.started": "2024-06-03T03:42:15.9886Z",
          "shell.execute_reply": "2024-06-03T03:42:16.014535Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Encode your categorical columns\n",
        "# Getting the categorical columns\n",
        "df_new[\"CSAT Score\"]=df_new[\"CSAT Score\"].astype('int')\n",
        "categorical_columns=list(set(df_new.columns.to_list()).difference(set(df_new.describe().columns.to_list())))\n",
        "non_cat_columns=['issue_responded','order_date_time','Issue_reported at','Survey_response_Date','Customer Remarks']\n",
        "categorical_columns = list(set(categorical_columns) - set(non_cat_columns))\n",
        "print(\"Categorical Columns are :-\", categorical_columns, \" :- \", len(categorical_columns))"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:16.016443Z",
          "iopub.execute_input": "2024-06-03T03:42:16.016728Z",
          "iopub.status.idle": "2024-06-03T03:42:16.053856Z",
          "shell.execute_reply.started": "2024-06-03T03:42:16.016705Z",
          "shell.execute_reply": "2024-06-03T03:42:16.052929Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform one-hot encoding\n",
        "df_encoded = pd.get_dummies(df_new, columns=categorical_columns)\n",
        "\n",
        "# Display the encoded DataFrame\n",
        "df_encoded.head()"
      ],
      "metadata": {
        "id": "axarb3YM1yvY",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:16.054995Z",
          "iopub.execute_input": "2024-06-03T03:42:16.055288Z",
          "iopub.status.idle": "2024-06-03T03:42:16.434618Z",
          "shell.execute_reply.started": "2024-06-03T03:42:16.055263Z",
          "shell.execute_reply": "2024-06-03T03:42:16.43362Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used One Hot Encoding for all the categorical features,because these features are likely nominal categorical variables, meaning there is no inherent order or ranking among the categories. For these variables, it would be appropriate to apply one-hot encoding."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Created Some new features like Response_Time_seconds,day_number_order_date,weekday_number_order_date,weekday_num_response_date and day_num_response_date"
      ],
      "metadata": {
        "id": "3NKywcI3So33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Ensure the 'Issue reported at' and 'Issue responded' columns are in datetime format\n",
        "df_encoded['Issue_reported at'] = pd.to_datetime(df_encoded['Issue_reported at'], format='%d/%m/%Y %H:%M')\n",
        "df_encoded['issue_responded'] = pd.to_datetime(df_encoded['issue_responded'], format='%d/%m/%Y %H:%M')\n",
        "\n",
        "\n",
        "# Create a new feature the response time\n",
        "df_encoded['Response_Time'] = df_encoded['issue_responded'] - df_encoded['Issue_reported at']\n",
        "\n",
        "# Convert 'Response_Time' to a numerical format in seconds for aggregation\n",
        "df_encoded['Response_Time_seconds'] = df_encoded['Response_Time'].dt.total_seconds()"
      ],
      "metadata": {
        "id": "Y8wIU7H9vwUa",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:16.435865Z",
          "iopub.execute_input": "2024-06-03T03:42:16.436169Z",
          "iopub.status.idle": "2024-06-03T03:42:17.328536Z",
          "shell.execute_reply.started": "2024-06-03T03:42:16.436144Z",
          "shell.execute_reply": "2024-06-03T03:42:17.327529Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert order_date_time to datetime\n",
        "df_encoded['order_date_time'] = pd.to_datetime(df_encoded['order_date_time'], format='%d/%m/%Y %H:%M')\n",
        "\n",
        "# Extract day number (day of the month)\n",
        "df_encoded['day_number_order_date'] = df_encoded['order_date_time'].dt.day\n",
        "\n",
        "# Extract weekday (numerical value: 0 for Sunday, 1 for Monday, etc.)\n",
        "df_encoded['weekday_num_order_date'] = df_encoded['order_date_time'].dt.weekday + 1  # Monday=1, Sunday=7\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Convert 'Survey_response_Date' to datetime format\n",
        "df_encoded['Survey_response_Date'] = pd.to_datetime(df_encoded['Survey_response_Date'], format='%d-%b-%y')\n",
        "\n",
        "# Extract day number (day of the month)\n",
        "df_encoded['day_number_response_date'] = df_encoded['Survey_response_Date'].dt.day\n",
        "\n",
        "# Extract weekday (numerical value: 0 for Sunday, 1 for Monday, etc.)\n",
        "df_encoded['weekday_num_response_date'] = df_encoded['Survey_response_Date'].dt.weekday + 1\n"
      ],
      "metadata": {
        "id": "2i-dR56MwCYg",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:17.330126Z",
          "iopub.execute_input": "2024-06-03T03:42:17.330492Z",
          "iopub.status.idle": "2024-06-03T03:42:17.446475Z",
          "shell.execute_reply.started": "2024-06-03T03:42:17.330457Z",
          "shell.execute_reply": "2024-06-03T03:42:17.445525Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop Date columns after feature extraction\n",
        "df_encoded.drop(columns=['order_date_time', 'Survey_response_Date','Issue_reported at','issue_responded','Response_Time'], inplace=True)"
      ],
      "metadata": {
        "id": "ZrPODWG2bZB4",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:17.447543Z",
          "iopub.execute_input": "2024-06-03T03:42:17.447838Z",
          "iopub.status.idle": "2024-06-03T03:42:17.551022Z",
          "shell.execute_reply.started": "2024-06-03T03:42:17.447813Z",
          "shell.execute_reply": "2024-06-03T03:42:17.549975Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded.head()"
      ],
      "metadata": {
        "id": "fkJniPCwdux5",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:17.552364Z",
          "iopub.execute_input": "2024-06-03T03:42:17.552658Z",
          "iopub.status.idle": "2024-06-03T03:42:17.579893Z",
          "shell.execute_reply.started": "2024-06-03T03:42:17.552622Z",
          "shell.execute_reply": "2024-06-03T03:42:17.578923Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the shape of dataset\n",
        "df_encoded.shape"
      ],
      "metadata": {
        "id": "PLWgA-MabkJR",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:17.581077Z",
          "iopub.execute_input": "2024-06-03T03:42:17.581473Z",
          "iopub.status.idle": "2024-06-03T03:42:17.587613Z",
          "shell.execute_reply.started": "2024-06-03T03:42:17.581427Z",
          "shell.execute_reply": "2024-06-03T03:42:17.586619Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping Constant and Quasi Constant Feature\n",
        "def dropping_constant(data):\n",
        "    from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "    # Drop non-numeric columns\n",
        "    numeric_data = data.select_dtypes(include=['number'])\n",
        "\n",
        "    var_thres = VarianceThreshold(threshold=0.05)\n",
        "    var_thres.fit(numeric_data)\n",
        "\n",
        "    concol = [column for column in numeric_data.columns\n",
        "              if column not in numeric_data.columns[var_thres.get_support()]]\n",
        "\n",
        "    if \"CSAT Score\" in concol:\n",
        "        concol.remove(\"CSAT Score\")\n",
        "\n",
        "    df_removed_var = data.drop(concol, axis=1)\n",
        "    return df_removed_var"
      ],
      "metadata": {
        "id": "9VRjOWckytqe",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:17.589018Z",
          "iopub.execute_input": "2024-06-03T03:42:17.589376Z",
          "iopub.status.idle": "2024-06-03T03:42:17.596766Z",
          "shell.execute_reply.started": "2024-06-03T03:42:17.589345Z",
          "shell.execute_reply": "2024-06-03T03:42:17.596027Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function\n",
        "df_removed_var=dropping_constant(df_encoded)"
      ],
      "metadata": {
        "id": "m76C7i9Ebx5o",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:17.598011Z",
          "iopub.execute_input": "2024-06-03T03:42:17.598498Z",
          "iopub.status.idle": "2024-06-03T03:42:17.952457Z",
          "shell.execute_reply.started": "2024-06-03T03:42:17.598472Z",
          "shell.execute_reply": "2024-06-03T03:42:17.951323Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the shape after feature dropped\n",
        "df_removed_var.shape"
      ],
      "metadata": {
        "id": "2037ugSmblqz",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:17.9543Z",
          "iopub.execute_input": "2024-06-03T03:42:17.95468Z",
          "iopub.status.idle": "2024-06-03T03:42:17.960848Z",
          "shell.execute_reply.started": "2024-06-03T03:42:17.954634Z",
          "shell.execute_reply": "2024-06-03T03:42:17.959766Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Drop non-numeric columns\n",
        "numeric_data = df_removed_var.select_dtypes(include=['number'])\n",
        "numeric_data.drop(columns=['CSAT Score'], inplace=True)\n",
        "corr = numeric_data.corr()\n",
        "cmap = cmap=sns.diverging_palette(5, 250, as_cmap=True)\n",
        "\n",
        "\n",
        "\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UeSGoW4V0LOm",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:17.962111Z",
          "iopub.execute_input": "2024-06-03T03:42:17.962404Z",
          "iopub.status.idle": "2024-06-03T03:42:18.482686Z",
          "shell.execute_reply.started": "2024-06-03T03:42:17.962381Z",
          "shell.execute_reply": "2024-06-03T03:42:18.481714Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install statsmodels\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_vif(df):\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"Feature\"] = numeric_data.columns\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(numeric_data.values, i) for i in range(numeric_data.shape[1])]\n",
        "    return vif_data\n",
        "\n",
        "# Assuming df is your DataFrame containing the features\n",
        "vif_results = calculate_vif(df)\n",
        "print(vif_results)\n"
      ],
      "metadata": {
        "id": "jjKkr7TC1XC3",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:18.483944Z",
          "iopub.execute_input": "2024-06-03T03:42:18.484255Z",
          "iopub.status.idle": "2024-06-03T03:42:33.672365Z",
          "shell.execute_reply.started": "2024-06-03T03:42:18.484228Z",
          "shell.execute_reply": "2024-06-03T03:42:33.669839Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop highly correlated feature\n",
        "df_removed_var.drop(columns=['weekday_num_order_date'], inplace=True)"
      ],
      "metadata": {
        "id": "u3hfIGLq4ZUL",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:33.675309Z",
          "iopub.execute_input": "2024-06-03T03:42:33.677091Z",
          "iopub.status.idle": "2024-06-03T03:42:33.859403Z",
          "shell.execute_reply.started": "2024-06-03T03:42:33.676908Z",
          "shell.execute_reply": "2024-06-03T03:42:33.858368Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Feature Correlation and finding multicolinearity\n",
        "def correlation(df,threshold):\n",
        "  col_corr=set()\n",
        "  corr_matrix= df.corr()\n",
        "  for i in range (len(corr_matrix.columns)):\n",
        "    for j in range(i):\n",
        "      if abs (corr_matrix.iloc[i,j])>threshold:\n",
        "        colname=corr_matrix.columns[i]\n",
        "        col_corr.add(colname)\n",
        "  return list(col_corr)\n",
        "\n"
      ],
      "metadata": {
        "id": "_RD2Mq1SAZM_",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:33.860943Z",
          "iopub.execute_input": "2024-06-03T03:42:33.861238Z",
          "iopub.status.idle": "2024-06-03T03:42:33.86924Z",
          "shell.execute_reply.started": "2024-06-03T03:42:33.861214Z",
          "shell.execute_reply": "2024-06-03T03:42:33.868284Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting multicolinear columns and dropping them\n",
        "numeric_data = df_removed_var.select_dtypes(include=['number'])\n",
        "numeric_data.drop(columns=['CSAT Score'], inplace=True)\n",
        "highly_correlated_columns=correlation(numeric_data,0.5)\n",
        "\n",
        "if \"CSAT Score\" in highly_correlated_columns:\n",
        "  highly_correlated_columns.remove(\"CSAT Score\")\n",
        "else:\n",
        "  pass\n",
        "\n",
        "df_removed=df_removed_var.drop(highly_correlated_columns,axis=1)\n",
        "df_removed.shape"
      ],
      "metadata": {
        "id": "f2HYgia8TCCa",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:33.870433Z",
          "iopub.execute_input": "2024-06-03T03:42:33.870766Z",
          "iopub.status.idle": "2024-06-03T03:42:33.996072Z",
          "shell.execute_reply.started": "2024-06-03T03:42:33.870743Z",
          "shell.execute_reply": "2024-06-03T03:42:33.994852Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_vif(df):\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"Feature\"] = numeric_data.columns\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(numeric_data.values, i) for i in range(numeric_data.shape[1])]\n",
        "    return vif_data\n",
        "\n",
        "# Assuming df is your DataFrame containing the features\n",
        "vif_results = calculate_vif(df)\n",
        "print(vif_results)"
      ],
      "metadata": {
        "id": "PGMxgebo5goC",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:33.998538Z",
          "iopub.execute_input": "2024-06-03T03:42:33.999563Z",
          "iopub.status.idle": "2024-06-03T03:42:34.316707Z",
          "shell.execute_reply.started": "2024-06-03T03:42:33.999528Z",
          "shell.execute_reply": "2024-06-03T03:42:34.315241Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After Feature Selection checking the shape left with\n",
        "df_removed.shape"
      ],
      "metadata": {
        "id": "T7_-rO-0fi9D",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:34.323269Z",
          "iopub.execute_input": "2024-06-03T03:42:34.325133Z",
          "iopub.status.idle": "2024-06-03T03:42:34.340389Z",
          "shell.execute_reply.started": "2024-06-03T03:42:34.325081Z",
          "shell.execute_reply": "2024-06-03T03:42:34.338888Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_removed.isnull().sum()"
      ],
      "metadata": {
        "id": "kr_li8kLPv96",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:34.348708Z",
          "iopub.execute_input": "2024-06-03T03:42:34.359844Z",
          "iopub.status.idle": "2024-06-03T03:42:34.649183Z",
          "shell.execute_reply.started": "2024-06-03T03:42:34.359794Z",
          "shell.execute_reply": "2024-06-03T03:42:34.647619Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Dropping Constant Feature, Dropping columns having multicolinearity and validate through VIF.\n",
        "\n",
        "Feature Selector that removes all low variance features. This feature selection algorithm looks only at the features(X), not the desired outputs(Y), and can be used for unsupported learning.\n",
        "\n",
        "A Pearson correlation is a number between -1 and 1 that indicates the extent to which two variables are linearly related. The Pearson correlation is also known as the product moment correlation coefficient (PMCC) or simply correlation\n",
        "\n",
        "Pearson correlations are suitable only for metric variables The correlation coefficient has values between -1 to 1\n",
        "\n",
        " A value closer to 0 implies weaker correlation (exact 0 implying no correlation)\n",
        "\n",
        " A value closer to 1 implies stronger positive correlation\n",
        "\n",
        " A value closer to -1 implies stronger negative correlation\n",
        "\n",
        "Collinearity is the state where two variables are highly correlated and contain similar information about the variance within a given dataset. To detect collinearity among variables, simply create a correlation matrix and find variables with large absolute values.\n",
        "\n",
        "Steps for Implementing VIF\n",
        "\n",
        " Calculate the VIF factors.\n",
        "\n",
        " Inspect the factors for each predictor variable, if the VIF is between 510, multicollinearity is likely present and you should consider dropping the variable.\n",
        "\n",
        "In VIF method, we pick each feature and regress it against all of the other features. For each regression, the factor is calculated as :\n",
        "\n",
        "VIF=\\frac{1}{1-R^2}\n",
        "\n",
        "Where, R-squared is the coefficient of determination in linear regression. Its value lies between 0 and 1.\n",
        "\n",
        "1st I dropped columns having constant or quasi constant variance. Then using pearson corelation I removed the columns having multicolinearity and again validate the VIFs for each feauture and found some features having VIF of more than 5-10 and I considered it to be 8 and again manipulated some features and again dropped multicolinear columns to make the VIF less than 8. The features got decreased from 77 to 10."
      ],
      "metadata": {
        "id": "8_syLORmZO_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting symmetric and skew symmetric features from the cplumns\n",
        "symmetric_feature=[]\n",
        "non_symmetric_feature=[]\n",
        "for i in df_removed.describe().columns:\n",
        "  if abs(df_removed[i].mean()-df_removed[i].median())<0.25:\n",
        "    symmetric_feature.append(i)\n",
        "  else:\n",
        "    non_symmetric_feature.append(i)\n",
        "\n",
        "# Getting Symmetric Distributed Features\n",
        "print(\"Symmetric Distributed Features : -\",symmetric_feature)\n",
        "# Removing Customer Service Calls column from the list as it's an important factor\n",
        "# which can't be treated as outliers here will is already leading to higher churn as we have seen furing analysis.\n",
        "non_symmetric_feature.remove('CSAT Score')\n",
        "\n",
        "# Getting Skew Symmetric Distributed Features\n",
        "print(\"Skew Symmetric Distributed Features : -\",non_symmetric_feature)"
      ],
      "metadata": {
        "id": "YnvslXZKj5dH",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:34.651774Z",
          "iopub.execute_input": "2024-06-03T03:42:34.652178Z",
          "iopub.status.idle": "2024-06-03T03:42:34.699491Z",
          "shell.execute_reply.started": "2024-06-03T03:42:34.652126Z",
          "shell.execute_reply": "2024-06-03T03:42:34.698533Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First Transformation**"
      ],
      "metadata": {
        "id": "1k6xA1sJN8S1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Exponential Transforming the required column\n",
        "df_removed['Item_price']=np.sqrt(df_removed['Item_price'])\n",
        "df_removed['Response_Time_seconds']=np.sqrt(df_removed['Response_Time_seconds'])\n",
        "df_removed['day_number_order_date']=(df_removed['day_number_order_date'])**0.25\n",
        "df_removed['day_number_response_date']=(df_removed['day_number_response_date'])**0.25\n"
      ],
      "metadata": {
        "id": "sac3ukURl6CW",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:34.700898Z",
          "iopub.execute_input": "2024-06-03T03:42:34.701189Z",
          "iopub.status.idle": "2024-06-03T03:42:34.710415Z",
          "shell.execute_reply.started": "2024-06-03T03:42:34.701165Z",
          "shell.execute_reply": "2024-06-03T03:42:34.709437Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_removed.isnull().sum()"
      ],
      "metadata": {
        "id": "BSAauNZrQNpw",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:34.711568Z",
          "iopub.execute_input": "2024-06-03T03:42:34.711873Z",
          "iopub.status.idle": "2024-06-03T03:42:34.956352Z",
          "shell.execute_reply.started": "2024-06-03T03:42:34.71185Z",
          "shell.execute_reply": "2024-06-03T03:42:34.95535Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fill NaN values with the median of Response_Time_seconds columns\n",
        "df_removed['Response_Time_seconds'] = df_removed['Response_Time_seconds'].fillna(df_removed['Response_Time_seconds'].median())\n"
      ],
      "metadata": {
        "id": "VVa7Y8MFQsct",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:34.95758Z",
          "iopub.execute_input": "2024-06-03T03:42:34.957929Z",
          "iopub.status.idle": "2024-06-03T03:42:34.966873Z",
          "shell.execute_reply.started": "2024-06-03T03:42:34.957904Z",
          "shell.execute_reply": "2024-06-03T03:42:34.965702Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting symmetric and skew symmetric features from the cplumns\n",
        "symmetric_feature=[]\n",
        "non_symmetric_feature=[]\n",
        "for i in df_removed.describe().columns:\n",
        "  if abs(df_removed[i].mean()-df_removed[i].median())<0.25:\n",
        "    symmetric_feature.append(i)\n",
        "  else:\n",
        "    non_symmetric_feature.append(i)\n",
        "\n",
        "# Getting Symmetric Distributed Features\n",
        "print(\"Symmetric Distributed Features : -\",symmetric_feature)\n",
        "# Removing Customer Service Calls column from the list as it's an important factor\n",
        "# which can't be treated as outliers here will is already leading to higher churn as we have seen furing analysis.\n",
        "non_symmetric_feature.remove('CSAT Score')\n",
        "\n",
        "# Getting Skew Symmetric Distributed Features\n",
        "print(\"Skew Symmetric Distributed Features : -\",non_symmetric_feature)"
      ],
      "metadata": {
        "id": "xbjk7nAfFjWX",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:34.968513Z",
          "iopub.execute_input": "2024-06-03T03:42:34.968947Z",
          "iopub.status.idle": "2024-06-03T03:42:35.023086Z",
          "shell.execute_reply.started": "2024-06-03T03:42:34.968913Z",
          "shell.execute_reply": "2024-06-03T03:42:35.022075Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Second Transformation**"
      ],
      "metadata": {
        "id": "_8Wrr1HIOCHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_removed['Response_Time_seconds'] = np.sqrt(df_removed['Response_Time_seconds'])\n",
        "df_removed['Item_price'] = (df_removed['Item_price'])**0.25"
      ],
      "metadata": {
        "id": "Y4Z5lNTpGLXE",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:35.024348Z",
          "iopub.execute_input": "2024-06-03T03:42:35.024671Z",
          "iopub.status.idle": "2024-06-03T03:42:35.03039Z",
          "shell.execute_reply.started": "2024-06-03T03:42:35.024624Z",
          "shell.execute_reply": "2024-06-03T03:42:35.029457Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting symmetric and skew symmetric features from the cplumns\n",
        "symmetric_feature=[]\n",
        "non_symmetric_feature=[]\n",
        "for i in df_removed.describe().columns:\n",
        "  if abs(df_removed[i].mean()-df_removed[i].median())<0.25:\n",
        "    symmetric_feature.append(i)\n",
        "  else:\n",
        "    non_symmetric_feature.append(i)\n",
        "\n",
        "# Getting Symmetric Distributed Features\n",
        "print(\"Symmetric Distributed Features : -\",symmetric_feature)\n",
        "# Removing Customer Service Calls column from the list as it's an important factor\n",
        "# which can't be treated as outliers here will is already leading to higher churn as we have seen furing analysis.\n",
        "non_symmetric_feature.remove('CSAT Score')\n",
        "\n",
        "# Getting Skew Symmetric Distributed Features\n",
        "print(\"Skew Symmetric Distributed Features : -\",non_symmetric_feature)"
      ],
      "metadata": {
        "id": "z3_Ui1H-GYaq",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:35.048694Z",
          "iopub.execute_input": "2024-06-03T03:42:35.04904Z",
          "iopub.status.idle": "2024-06-03T03:42:35.097328Z",
          "shell.execute_reply.started": "2024-06-03T03:42:35.049015Z",
          "shell.execute_reply": "2024-06-03T03:42:35.096329Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Third Transformation**"
      ],
      "metadata": {
        "id": "qyhubR3sOJtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform sqrt transform on 'Response_Time_seconds' column\n",
        "df_removed['Response_Time_seconds'] = np.sqrt(df_removed['Response_Time_seconds'])"
      ],
      "metadata": {
        "id": "wsOd9la4GmaJ",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:35.098814Z",
          "iopub.execute_input": "2024-06-03T03:42:35.099517Z",
          "iopub.status.idle": "2024-06-03T03:42:35.104508Z",
          "shell.execute_reply.started": "2024-06-03T03:42:35.099482Z",
          "shell.execute_reply": "2024-06-03T03:42:35.103626Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting symmetric and skew symmetric features from the cplumns\n",
        "symmetric_feature=[]\n",
        "non_symmetric_feature=[]\n",
        "for i in df_removed.describe().columns:\n",
        "  if abs(df_removed[i].mean()-df_removed[i].median())<0.25:\n",
        "    symmetric_feature.append(i)\n",
        "  else:\n",
        "    non_symmetric_feature.append(i)\n",
        "\n",
        "# Getting Symmetric Distributed Features\n",
        "print(\"Symmetric Distributed Features : -\",symmetric_feature)\n",
        "# Removing Customer Service Calls column from the list as it's an important factor\n",
        "# which can't be treated as outliers here will is already leading to higher churn as we have seen furing analysis.\n",
        "non_symmetric_feature.remove('CSAT Score')\n",
        "\n",
        "# Getting Skew Symmetric Distributed Features\n",
        "print(\"Skew Symmetric Distributed Features : -\",non_symmetric_feature)"
      ],
      "metadata": {
        "id": "qcplCg-fGueS",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:35.105678Z",
          "iopub.execute_input": "2024-06-03T03:42:35.105992Z",
          "iopub.status.idle": "2024-06-03T03:42:35.155059Z",
          "shell.execute_reply.started": "2024-06-03T03:42:35.105969Z",
          "shell.execute_reply": "2024-06-03T03:42:35.154071Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing code of hist plot for each columns to know the data distibution\n",
        "for col in df_removed.loc[:,symmetric_feature]:\n",
        "  fig=plt.figure(figsize=(9,6))\n",
        "  ax=fig.gca()\n",
        "  feature= (df_removed[col])\n",
        "  sns.distplot(df_removed[col])\n",
        "  ax.axvline(feature.mean(),color='magenta', linestyle='dashed', linewidth=2)\n",
        "  ax.axvline(feature.median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "  ax.set_title(col)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zdFioDnFkscQ",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:35.15646Z",
          "iopub.execute_input": "2024-06-03T03:42:35.156912Z",
          "iopub.status.idle": "2024-06-03T03:42:39.538656Z",
          "shell.execute_reply.started": "2024-06-03T03:42:35.156863Z",
          "shell.execute_reply": "2024-06-03T03:42:39.537749Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the features, I got to know that there are 2 features which aren't symmetric so aren't following gaussian distribution and rest are having szymmetric curve. Thus, for those two columns I have used Exponential transformation to achieve gaussian distribution.\n",
        "\n",
        " I tried with other transformations and found exponetial tranformation with no infinity value and working fine. So, I am continuing with Exponentia lransformation with a power of 0.25.\n"
      ],
      "metadata": {
        "id": "u8DJwJSybwe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# Checking the data\n",
        "df_removed.head()"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:39.539917Z",
          "iopub.execute_input": "2024-06-03T03:42:39.540242Z",
          "iopub.status.idle": "2024-06-03T03:42:39.56847Z",
          "shell.execute_reply.started": "2024-06-03T03:42:39.540204Z",
          "shell.execute_reply": "2024-06-03T03:42:39.567534Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df=df_removed.copy()"
      ],
      "metadata": {
        "id": "5tFjtjMXVzCK",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:39.569899Z",
          "iopub.execute_input": "2024-06-03T03:42:39.57032Z",
          "iopub.status.idle": "2024-06-03T03:42:39.873614Z",
          "shell.execute_reply.started": "2024-06-03T03:42:39.570285Z",
          "shell.execute_reply": "2024-06-03T03:42:39.872524Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=df_removed['CSAT Score']"
      ],
      "metadata": {
        "id": "FL9z4CopWyT6",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:39.874911Z",
          "iopub.execute_input": "2024-06-03T03:42:39.875236Z",
          "iopub.status.idle": "2024-06-03T03:42:39.879682Z",
          "shell.execute_reply.started": "2024-06-03T03:42:39.87521Z",
          "shell.execute_reply": "2024-06-03T03:42:39.878708Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(y)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:39.881088Z",
          "iopub.execute_input": "2024-06-03T03:42:39.881756Z",
          "iopub.status.idle": "2024-06-03T03:42:39.894495Z",
          "shell.execute_reply.started": "2024-06-03T03:42:39.881722Z",
          "shell.execute_reply": "2024-06-03T03:42:39.893601Z"
        },
        "trusted": true,
        "id": "XnUw07nBmGJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_removed.drop(columns=['CSAT Score'],inplace=True)"
      ],
      "metadata": {
        "id": "jpEgHwGeXDKC",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:39.89562Z",
          "iopub.execute_input": "2024-06-03T03:42:39.895914Z",
          "iopub.status.idle": "2024-06-03T03:42:39.99659Z",
          "shell.execute_reply.started": "2024-06-03T03:42:39.89589Z",
          "shell.execute_reply": "2024-06-03T03:42:39.995828Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_removed.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:39.997691Z",
          "iopub.execute_input": "2024-06-03T03:42:39.997979Z",
          "iopub.status.idle": "2024-06-03T03:42:40.025191Z",
          "shell.execute_reply.started": "2024-06-03T03:42:39.997955Z",
          "shell.execute_reply": "2024-06-03T03:42:40.024126Z"
        },
        "trusted": true,
        "id": "Cg7HHTK-mGJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select only the numerical columns from df_removed\n",
        "numerical_columns = df_removed.select_dtypes(include=['number']).columns\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Apply the scaler to the numerical columns\n",
        "df_removed[numerical_columns] = scaler.fit_transform(df_removed[numerical_columns])\n",
        "\n",
        "# Save the fitted scaler\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "# Display the scaled DataFrame\n",
        "df_removed.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "UwNwBsSjVnwE",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:40.026437Z",
          "iopub.execute_input": "2024-06-03T03:42:40.026743Z",
          "iopub.status.idle": "2024-06-03T03:42:40.078433Z",
          "shell.execute_reply.started": "2024-06-03T03:42:40.026719Z",
          "shell.execute_reply": "2024-06-03T03:42:40.077502Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_columns"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:40.079527Z",
          "iopub.execute_input": "2024-06-03T03:42:40.07983Z",
          "iopub.status.idle": "2024-06-03T03:42:40.085881Z",
          "shell.execute_reply.started": "2024-06-03T03:42:40.079806Z",
          "shell.execute_reply": "2024-06-03T03:42:40.085Z"
        },
        "trusted": true,
        "id": "tNl4WTOPmGJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fitted scaler\n",
        "joblib.dump(scaler, \"scaler.pkl\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:40.087026Z",
          "iopub.execute_input": "2024-06-03T03:42:40.087288Z",
          "iopub.status.idle": "2024-06-03T03:42:40.099386Z",
          "shell.execute_reply.started": "2024-06-03T03:42:40.087266Z",
          "shell.execute_reply": "2024-06-03T03:42:40.09856Z"
        },
        "trusted": true,
        "id": "neve6MaQmGJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "d88g4MramGJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_removed.isnull().sum()"
      ],
      "metadata": {
        "id": "ygyUGCuzPjZt",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:40.100564Z",
          "iopub.execute_input": "2024-06-03T03:42:40.101357Z",
          "iopub.status.idle": "2024-06-03T03:42:40.336237Z",
          "shell.execute_reply.started": "2024-06-03T03:42:40.101326Z",
          "shell.execute_reply": "2024-06-03T03:42:40.335248Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you are using an algorithm that assumes your features have a similar range, you should use feature scaling.\n",
        "\n",
        "If the ranges of your features differ much then you should use feature scaling. If the range does not vary a lot like one of them is between 0 and 2 and the other one is between -1 and 0.5 then you can leave them as it's. However, you should use feature scaling if the ranges are, for example, between -2 and 2 and between -100 and 100.\n",
        "\n",
        "Use Standardization when your data follows Gaussian distribution.\n",
        "Use Normalization when your data does not follow Gaussian distribution.\n",
        "\n",
        "So, in my data only Account Length column having large data difference and following gaussian distribution. That's why, I have used standardization using atandardscaler.\n"
      ],
      "metadata": {
        "id": "-_-SaQemdYeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)\n",
        "\n"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Importing necessary libraries for text preprocessing**"
      ],
      "metadata": {
        "id": "A1tXasLFv6Dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "SfKGJdnkv5F8",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:40.337561Z",
          "iopub.execute_input": "2024-06-03T03:42:40.337976Z",
          "iopub.status.idle": "2024-06-03T03:42:41.727565Z",
          "shell.execute_reply.started": "2024-06-03T03:42:40.337947Z",
          "shell.execute_reply": "2024-06-03T03:42:41.726532Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.data.path"
      ],
      "metadata": {
        "id": "MCUHUPfaeP_n",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:41.728757Z",
          "iopub.execute_input": "2024-06-03T03:42:41.729034Z",
          "iopub.status.idle": "2024-06-03T03:42:41.734794Z",
          "shell.execute_reply.started": "2024-06-03T03:42:41.72901Z",
          "shell.execute_reply": "2024-06-03T03:42:41.733928Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "# Function to expand contractions\n",
        "def expand_contractions(text):\n",
        "    # Dictionary of English contractions\n",
        "    contractions = {\n",
        "        \"ain't\": \"am not\",\n",
        "        \"aren't\": \"are not\",\n",
        "        \"can't\": \"cannot\",\n",
        "        \"could've\": \"could have\",\n",
        "        \"couldn't\": \"could not\",\n",
        "        \"didn't\": \"did not\",\n",
        "        \"doesn't\": \"does not\",\n",
        "        \"don't\": \"do not\",\n",
        "        \"hadn't\": \"had not\",\n",
        "        \"hasn't\": \"has not\",\n",
        "        \"haven't\": \"have not\",\n",
        "        \"he'd\": \"he would\",\n",
        "        \"he'll\": \"he will\",\n",
        "        \"he's\": \"he is\",\n",
        "        \"how'd\": \"how did\",\n",
        "        \"how'll\": \"how will\",\n",
        "        \"how's\": \"how is\",\n",
        "        \"i'd\": \"i would\",\n",
        "        \"i'll\": \"i will\",\n",
        "        \"i'm\": \"i am\",\n",
        "        \"i've\": \"i have\",\n",
        "        \"isn't\": \"is not\",\n",
        "        \"it'd\": \"it would\",\n",
        "        \"it'll\": \"it will\",\n",
        "        \"it's\": \"it is\",\n",
        "        \"let's\": \"let us\",\n",
        "        \"might've\": \"might have\",\n",
        "        \"must've\": \"must have\",\n",
        "        \"shan't\": \"shall not\",\n",
        "        \"she'd\": \"she would\",\n",
        "        \"she'll\": \"she will\",\n",
        "        \"she's\": \"she is\",\n",
        "        \"should've\": \"should have\",\n",
        "        \"shouldn't\": \"should not\",\n",
        "        \"that'd\": \"that would\",\n",
        "        \"that's\": \"that is\",\n",
        "        \"there's\": \"there is\",\n",
        "        \"they'd\": \"they would\",\n",
        "        \"they'll\": \"they will\",\n",
        "        \"they're\": \"they are\",\n",
        "        \"they've\": \"they have\",\n",
        "        \"wasn't\": \"was not\",\n",
        "        \"we'd\": \"we would\",\n",
        "        \"we'll\": \"we will\",\n",
        "        \"we're\": \"we are\",\n",
        "        \"we've\": \"we have\",\n",
        "        \"weren't\": \"were not\",\n",
        "        \"what'll\": \"what will\",\n",
        "        \"what're\": \"what are\",\n",
        "        \"what's\": \"what is\",\n",
        "        \"what've\": \"what have\",\n",
        "        \"when's\": \"when is\",\n",
        "        \"where'd\": \"where did\",\n",
        "        \"where's\": \"where is\",\n",
        "        \"who'll\": \"who will\",\n",
        "        \"who's\": \"who is\",\n",
        "        \"who've\": \"who have\",\n",
        "        \"why's\": \"why is\",\n",
        "        \"won't\": \"will not\",\n",
        "        \"would've\": \"would have\",\n",
        "        \"wouldn't\": \"would not\",\n",
        "        \"y'all\": \"you all\",\n",
        "        \"you'd\": \"you would\",\n",
        "        \"you'll\": \"you will\",\n",
        "        \"you're\": \"you are\",\n",
        "        \"you've\": \"you have\"\n",
        "    }\n",
        "    # Regular expression pattern to match contractions\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(contractions.keys()) + r')\\b')\n",
        "    # Replace contractions with their expansions\n",
        "    expanded_text = pattern.sub(lambda match: contractions[match.group(0)], text)\n",
        "    return expanded_text\n",
        "#Apply text preprocessing to the 'Customer Remarks' feature\n",
        "df_removed['Customer Remarks'] = df_removed['Customer Remarks'].apply(expand_contractions)"
      ],
      "metadata": {
        "id": "PTouz10C3oNN",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:41.736343Z",
          "iopub.execute_input": "2024-06-03T03:42:41.736611Z",
          "iopub.status.idle": "2024-06-03T03:42:43.00575Z",
          "shell.execute_reply.started": "2024-06-03T03:42:41.736589Z",
          "shell.execute_reply": "2024-06-03T03:42:43.004698Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "df_removed['Customer Remarks'] = df_removed['Customer Remarks'].str.lower()"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:43.006998Z",
          "iopub.execute_input": "2024-06-03T03:42:43.007304Z",
          "iopub.status.idle": "2024-06-03T03:42:43.035607Z",
          "shell.execute_reply.started": "2024-06-03T03:42:43.007278Z",
          "shell.execute_reply": "2024-06-03T03:42:43.0349Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "df_removed['Customer Remarks'] = df_removed['Customer Remarks'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:43.036843Z",
          "iopub.execute_input": "2024-06-03T03:42:43.037123Z",
          "iopub.status.idle": "2024-06-03T03:42:43.263925Z",
          "shell.execute_reply.started": "2024-06-03T03:42:43.037099Z",
          "shell.execute_reply": "2024-06-03T03:42:43.263092Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs\n",
        "# Function to remove URLs\n",
        "def remove_urls(text):\n",
        "    # Regular expression pattern to match URLs\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    # Remove URLs from the text\n",
        "    cleaned_text = url_pattern.sub('', text)\n",
        "    return cleaned_text\n",
        "\n",
        "df_removed['Customer Remarks'] = df_removed['Customer Remarks'].apply(remove_urls)"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:43.265106Z",
          "iopub.execute_input": "2024-06-03T03:42:43.265426Z",
          "iopub.status.idle": "2024-06-03T03:42:43.454421Z",
          "shell.execute_reply.started": "2024-06-03T03:42:43.265402Z",
          "shell.execute_reply": "2024-06-03T03:42:43.453622Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove words containing digits\n",
        "def remove_digits(text):\n",
        "    # Regular expression pattern to match words containing digits\n",
        "    digit_pattern = re.compile(r'\\w*\\d\\w*')\n",
        "    # Remove words containing digits from the text\n",
        "    cleaned_text = digit_pattern.sub('', text)\n",
        "    return cleaned_text\n",
        "\n",
        "df_removed['Customer Remarks'] = df_removed['Customer Remarks'].apply(remove_digits)"
      ],
      "metadata": {
        "id": "xGLu2oXkzO47",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:43.455585Z",
          "iopub.execute_input": "2024-06-03T03:42:43.455922Z",
          "iopub.status.idle": "2024-06-03T03:42:44.056742Z",
          "shell.execute_reply.started": "2024-06-03T03:42:43.455895Z",
          "shell.execute_reply": "2024-06-03T03:42:44.055695Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing Stopwords\n",
        "\n",
        "# Get the English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stopwords without tokenization\n",
        "def remove_stopwords(text):\n",
        "    filtered_tokens = [word for word in text.split() if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "# Remove stopwords\n",
        "df_removed['Customer Remarks'] = df_removed['Customer Remarks'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "MGgxjEaj20Pv",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:44.057922Z",
          "iopub.execute_input": "2024-06-03T03:42:44.058257Z",
          "iopub.status.idle": "2024-06-03T03:42:44.242055Z",
          "shell.execute_reply.started": "2024-06-03T03:42:44.058232Z",
          "shell.execute_reply": "2024-06-03T03:42:44.241067Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing White spaces\n",
        "df_removed['Customer Remarks'] = df_removed['Customer Remarks'].str.strip()"
      ],
      "metadata": {
        "id": "nDcIHyf2zozT",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:44.243323Z",
          "iopub.execute_input": "2024-06-03T03:42:44.243612Z",
          "iopub.status.idle": "2024-06-03T03:42:44.274136Z",
          "shell.execute_reply.started": "2024-06-03T03:42:44.243588Z",
          "shell.execute_reply": "2024-06-03T03:42:44.273097Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "\n",
        "# Tokenize text\n",
        "df_removed['Customer Remarks'] = df_removed['Customer Remarks'].apply(nltk.word_tokenize)"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:44.275439Z",
          "iopub.execute_input": "2024-06-03T03:42:44.275933Z",
          "iopub.status.idle": "2024-06-03T03:42:53.610003Z",
          "shell.execute_reply.started": "2024-06-03T03:42:44.275898Z",
          "shell.execute_reply": "2024-06-03T03:42:53.608961Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import subprocess\n",
        "\n",
        "# Download and unzip wordnet\n",
        "try:\n",
        "    nltk.data.find('wordnet.zip')\n",
        "except:\n",
        "    nltk.download('wordnet', download_dir='/kaggle/working/')\n",
        "    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n",
        "    subprocess.run(command.split())\n",
        "    nltk.data.path.append('/kaggle/working/')\n",
        "\n",
        "# Now you can import the NLTK resources as usual\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:53.611282Z",
          "iopub.execute_input": "2024-06-03T03:42:53.613235Z",
          "iopub.status.idle": "2024-06-03T03:42:54.096597Z",
          "shell.execute_reply.started": "2024-06-03T03:42:53.6132Z",
          "shell.execute_reply": "2024-06-03T03:42:54.095488Z"
        },
        "trusted": true,
        "id": "deZmXqhKmGJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "# Initialize the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "dDgR6yWtMno-",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:54.09786Z",
          "iopub.execute_input": "2024-06-03T03:42:54.098539Z",
          "iopub.status.idle": "2024-06-03T03:42:54.103259Z",
          "shell.execute_reply.started": "2024-06-03T03:42:54.098502Z",
          "shell.execute_reply": "2024-06-03T03:42:54.102199Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for text normalization\n",
        "def normalize_text(text):\n",
        "\n",
        "    # Check if input is a list\n",
        "    if isinstance(text, list):\n",
        "        # Initialize an empty list to store normalized text\n",
        "        normalized_texts = []\n",
        "        # Iterate over each element in the list\n",
        "        for item in text:\n",
        "            # Lemmatize each word and convert to lowercase\n",
        "            normalized_words = [lemmatizer.lemmatize(word.lower()) for word in item.split()]\n",
        "            # Join the normalized words back into a string\n",
        "            normalized_text = ' '.join(normalized_words)\n",
        "            # Remove non-alphanumeric characters\n",
        "            normalized_text = re.sub(r'[^a-zA-Z0-9\\s]', '', normalized_text)\n",
        "            # Append the normalized text to the list\n",
        "            normalized_texts.append(normalized_text)\n",
        "        return normalized_texts\n",
        "    else:\n",
        "        # Lemmatize each word and convert to lowercase\n",
        "        normalized_words = [lemmatizer.lemmatize(word.lower()) for word in text.split()]\n",
        "        # Join the normalized words back into a string\n",
        "        normalized_text = ' '.join(normalized_words)\n",
        "        # Remove non-alphanumeric characters\n",
        "        normalized_text = re.sub(r'[^a-zA-Z0-9\\s]', '', normalized_text)\n",
        "        return normalized_text\n",
        "\n",
        "# Apply text normalization to the 'Customer Remarks' column\n",
        "df_removed['Customer Remarks'] = df_removed['Customer Remarks'].apply(normalize_text)\n"
      ],
      "metadata": {
        "id": "9xl9YJbj4RBG",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:54.104613Z",
          "iopub.execute_input": "2024-06-03T03:42:54.104985Z",
          "iopub.status.idle": "2024-06-03T03:42:58.580425Z",
          "shell.execute_reply.started": "2024-06-03T03:42:54.104954Z",
          "shell.execute_reply": "2024-06-03T03:42:58.579618Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " I used lemmatization and removal of non-alphanumeric characters for text normalization.\n",
        "\n",
        "**Lemmatization:** Lemmatization reduces words to their base or root form, which helps in standardizing variations of words. For example, words like \"running\", \"ran\", and \"runs\" all reduce to the base form \"run\". This technique ensures that different forms of the same word are treated as identical, which can improve the effectiveness of text analysis tasks such as sentiment analysis or topic modeling.\n",
        "\n",
        "**Removal of Non-Alphanumeric Characters:** This step removes any characters that are not letters or numbers from the text. Non-alphanumeric characters, such as punctuation marks and special symbols, do not typically carry meaningful information for many natural language processing tasks. Removing them helps to simplify the text and focus on the essential content, improving the efficiency of subsequent analyses.\n",
        "\n",
        "By employing lemmatization and removal of non-alphanumeric characters, the text normalization process aims to standardize and clean the textual data, making it more suitable for further analysis or modeling."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Convert list of strings to a single string\n",
        "df_removed['Customer Remarks'] = df_removed['Customer Remarks'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the 'Customer Remarks' column\n",
        "customer_remarks_tfidf = tfidf_vectorizer.fit_transform(df_removed['Customer Remarks'])\n",
        "\n",
        "# Convert the vectors to an array\n",
        "customer_remarks_tfidf_array = customer_remarks_tfidf.toarray()\n",
        "\n",
        "# Get the feature names (vocabulary)\n",
        "feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a DataFrame for the TF-IDF vectors\n",
        "customer_remarks_tfidf_df = pd.DataFrame(customer_remarks_tfidf_array, columns=feature_names_tfidf)\n"
      ],
      "metadata": {
        "id": "ldFAtk-B4034",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:42:58.581743Z",
          "iopub.execute_input": "2024-06-03T03:42:58.582192Z",
          "iopub.status.idle": "2024-06-03T03:43:00.312208Z",
          "shell.execute_reply.started": "2024-06-03T03:42:58.582159Z",
          "shell.execute_reply": "2024-06-03T03:43:00.311414Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the index of 'df_removed' for proper concatenation\n",
        "#df_removed.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Concatenate the DataFrames along the columns axis\n",
        "#df_combined = pd.concat([df_removed, customer_remarks_tfidf_df], axis=1)\n",
        "\n",
        "# Now 'df_combined' contains all the columns from 'df_removed' and the TF-IDF vectors as additional features"
      ],
      "metadata": {
        "id": "7WgAYBmf5l4V",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:43:00.313514Z",
          "iopub.execute_input": "2024-06-03T03:43:00.314145Z",
          "iopub.status.idle": "2024-06-03T03:43:00.318853Z",
          "shell.execute_reply.started": "2024-06-03T03:43:00.314101Z",
          "shell.execute_reply": "2024-06-03T03:43:00.317777Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_combined.shape"
      ],
      "metadata": {
        "id": "FEycf-Kx6CEJ",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:43:00.320049Z",
          "iopub.execute_input": "2024-06-03T03:43:00.320379Z",
          "iopub.status.idle": "2024-06-03T03:43:00.334972Z",
          "shell.execute_reply.started": "2024-06-03T03:43:00.320343Z",
          "shell.execute_reply": "2024-06-03T03:43:00.334188Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code provided, I used the TF-IDF (Term Frequency-Inverse Document Frequency) vectorization technique.\n",
        "\n",
        "TF-IDF is a popular text vectorization technique that reflects the importance of a word in a document relative to a collection of documents (corpus). It considers both the frequency of the word in the document (TF) and the rarity of the word across the entire corpus (IDF). Words that appear frequently in a document but rarely in other documents are given higher weights.\n",
        "\n",
        "I chose TF-IDF vectorization because it helps capture the importance of words in the 'Customer Remarks' column while also reducing the impact of commonly occurring words across all remarks. This technique is suitable for tasks such as text classification, clustering, and sentiment analysis, where it's important to identify unique features in the text data."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_removed.drop(columns=['Customer Remarks'], inplace=True)"
      ],
      "metadata": {
        "id": "UD6k_VDVTc4B",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:43:00.336036Z",
          "iopub.execute_input": "2024-06-03T03:43:00.33638Z",
          "iopub.status.idle": "2024-06-03T03:43:00.439208Z",
          "shell.execute_reply.started": "2024-06-03T03:43:00.336342Z",
          "shell.execute_reply": "2024-06-03T03:43:00.438383Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#One Hot Encoding of Target Variable\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Extract the target variable\n",
        "y = final_df['CSAT Score'].values.reshape(-1, 1)\n",
        "\n",
        "# Initialize the OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "# Fit and transform the target variable\n",
        "y_one_hot = encoder.fit_transform(y)\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "y_one_hot_df = pd.DataFrame(y_one_hot, columns=[f'class_{int(i)}' for i in range(y_one_hot.shape[1])])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T03:43:00.440218Z",
          "iopub.execute_input": "2024-06-03T03:43:00.440483Z",
          "iopub.status.idle": "2024-06-03T03:43:00.452481Z",
          "shell.execute_reply.started": "2024-06-03T03:43:00.440461Z",
          "shell.execute_reply": "2024-06-03T03:43:00.451672Z"
        },
        "trusted": true,
        "id": "KME4sLzKmGJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        " # split into 70:30 ratio\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_removed,y_one_hot_df, test_size = 0.3, random_state = 0)\n",
        "\n",
        "# describes info about train and test set\n",
        "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
        "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
        "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
        "print(\"Number transactions y_test dataset: \", y_test.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T03:43:00.453747Z",
          "iopub.execute_input": "2024-06-03T03:43:00.454089Z",
          "iopub.status.idle": "2024-06-03T03:43:00.918828Z",
          "shell.execute_reply.started": "2024-06-03T03:43:00.454055Z",
          "shell.execute_reply": "2024-06-03T03:43:00.917818Z"
        },
        "trusted": true,
        "id": "N3k5yZVBmGJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_list=X_train.columns.to_list()\n",
        "import joblib\n",
        "# Save the fitted scaler\n",
        "joblib.dump(features_list, \"features.pkl\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T03:43:00.920178Z",
          "iopub.execute_input": "2024-06-03T03:43:00.920868Z",
          "iopub.status.idle": "2024-06-03T03:43:00.949809Z",
          "shell.execute_reply.started": "2024-06-03T03:43:00.92083Z",
          "shell.execute_reply": "2024-06-03T03:43:00.948874Z"
        },
        "trusted": true,
        "id": "5scUHUVtmGJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two competing concerns: with less training data, your parameter estimates have greater variance. With less testing data, your performance statistic will have greater variance. Broadly speaking you should be concerned with dividing data such that neither variance is too high, which is more to do with the absolute number of instances in each category rather than the percentage.\n",
        "\n",
        "If you have a total of 100 instances, you're probably stuck with cross validation as no single split is going to give you satisfactory variance in your estimates. If you have 100,000 instances, it doesn't really matter whether you choose an 80:20 split or a 90:10 split (indeed you may choose to use less training data if your method is particularly computationally intensive).\n",
        "\n",
        "You'd be surprised to find out that 80/20 is quite a commonly occurring ratio, often referred to as the Pareto principle. It's usually a safe bet if you use that ratio.\n",
        "\n",
        "In this case the training dataset is small, that's why I have taken 70:30 ratio."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Dependant Column Value Counts\n",
        "print(y_train.value_counts())\n",
        "print(\" \")\n",
        "# Dependant Variable Column Visualization\n",
        "y_train.value_counts().plot(kind='pie',\n",
        "                              figsize=(15,6),\n",
        "                               labels=['5','1','4','3','2']\n",
        "\n",
        "\n",
        "                              )"
      ],
      "metadata": {
        "id": "nWaf9g7F3ZRX",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:43:00.950991Z",
          "iopub.execute_input": "2024-06-03T03:43:00.951249Z",
          "iopub.status.idle": "2024-06-03T03:43:01.120031Z",
          "shell.execute_reply.started": "2024-06-03T03:43:00.951227Z",
          "shell.execute_reply": "2024-06-03T03:43:01.118583Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imbalanced dataset is relevant primarily in the context of supervised machine learning involving two or more classes.\n",
        "\n",
        "Imbalance means that the number of data points available for different the classes is different:\n",
        "If there are two classes, then balanced data would mean 50% points for each of the class. For most machine learning techniques, little imbalance is not a problem. So, if there are 60% points for one class and 40% for the other class, it should not cause any significant performance degradation. Only when the class imbalance is high, e.g. 90% points for one class and 10% for the other, standard optimization criteria or performance measures may not be as effective and would need modification.\n",
        "\n",
        "In our case the dataset dependent column data ratio is 85:15. So, during model creating it's obvios that there will be bias and having a great chance of predicting the majority one so frequently. SO the dataset should be balanced before it going for the model creation part."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn==0.8.0\n"
      ],
      "metadata": {
        "id": "hS6xePZNTal1",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:43:01.122011Z",
          "iopub.execute_input": "2024-06-03T03:43:01.122731Z",
          "iopub.status.idle": "2024-06-03T03:43:14.640011Z",
          "shell.execute_reply.started": "2024-06-03T03:43:01.122682Z",
          "shell.execute_reply": "2024-06-03T03:43:14.638998Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalance in the Target Variable using S.M.O.T.E\n",
        "\n",
        "# Convert the one-hot encoded DataFrame back to a Series of original class labels to apply SMOTE\n",
        "y_series = y_train.idxmax(axis=1).apply(lambda x: int(x.split('_')[1]))\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Create an instance of SMOTE\n",
        "sm = SMOTE(random_state=42)\n",
        "\n",
        "# Resample the training data using SMOTE\n",
        "X_train_resampled, y_train_resampled = sm.fit_resample(X_train, y_series)\n",
        "\n",
        "# Describe info about train and test set\n",
        "print(\"Number of transactions in X_train dataset: \", X_train_resampled.shape)\n",
        "print(\"Number of transactions in y_train dataset: \", y_train_resampled.shape)\n",
        "print(\"Number of transactions in X_test dataset: \", X_test.shape)\n",
        "print(\"Number of transactions in y_test dataset: \", y_test.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T03:43:14.641484Z",
          "iopub.execute_input": "2024-06-03T03:43:14.641794Z",
          "iopub.status.idle": "2024-06-03T03:43:44.969093Z",
          "shell.execute_reply.started": "2024-06-03T03:43:14.641766Z",
          "shell.execute_reply": "2024-06-03T03:43:44.96801Z"
        },
        "trusted": true,
        "id": "TIN92JdAmGJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting the target variable train data shape that of test data shape\n",
        "\n",
        "# Extract the target variable\n",
        "y = y_train_resampled.values.reshape(-1, 1)\n",
        "\n",
        "# Initialize the OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "# Fit and transform the target variable\n",
        "y_one_hot = encoder.fit_transform(y)\n",
        "\n",
        "# Convert y_train_resampled_one_hot back to DataFrame for consistency\n",
        "y_train_resampled_df = pd.DataFrame(y_one_hot, columns=[f'class_{int(i)}' for i in range(y_one_hot.shape[1])])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T03:43:44.970246Z",
          "iopub.execute_input": "2024-06-03T03:43:44.970556Z",
          "iopub.status.idle": "2024-06-03T03:43:45.000894Z",
          "shell.execute_reply.started": "2024-06-03T03:43:44.97053Z",
          "shell.execute_reply": "2024-06-03T03:43:44.999927Z"
        },
        "trusted": true,
        "id": "IqChRgtCmGJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Dependant Column Value Counts\n",
        "print(y_train_resampled_df.value_counts())\n",
        "print(\" \")\n",
        "# Dependant Variable Column Visualization\n",
        "y_train_resampled_df.value_counts().plot(kind='pie',\n",
        "                              figsize=(15,6),\n",
        "                               labels=['1','2','3','4','5']\n",
        "\n",
        "\n",
        "                              )"
      ],
      "metadata": {
        "id": "nia0uhiMTwZo",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:43:45.002183Z",
          "iopub.execute_input": "2024-06-03T03:43:45.002485Z",
          "iopub.status.idle": "2024-06-03T03:43:45.190995Z",
          "shell.execute_reply.started": "2024-06-03T03:43:45.002459Z",
          "shell.execute_reply": "2024-06-03T03:43:45.189692Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used SMOTE (Synthetic Minority Over-sampling technique) for balanced the 85:15 dataset.\n",
        "\n",
        "SMOTE is a technique in machine learning for dealing with issues that arise when working with an unbalanced data set. In practice, unbalanced data sets are common and most ML algorithms are highly prone to unbalanced data so we need to improve their performance by using techniques like SMOTE.\n",
        "\n",
        "To address this disparity, balancing schemes that augment the data to make it more balanced before training the classifier were proposed. Oversampling the minority class by duplicating minority samples or undersampling the majority class is the simplest balancing method.\n",
        "\n",
        "The idea of incorporating synthetic minority samples into tabular data was first proposed in SMOTE, where synthetic minority samples are generated by interpolating pairs of original minority points.\n",
        "\n",
        "SMOTE is a data augmentation algorithm that creates synthetic data points from raw data. SMOTE can be thought of as a more sophisticated version of oversampling or a specific data augmentation algorithm.\n",
        "\n",
        "SMOTE has the advantage of not creating duplicate data points, but rather synthetic data points that differ slightly from the original data points. SMOTE is a superior oversampling option.\n",
        "\n",
        "That's why for lots of advantages, I have used SMOTE technique for balancinmg the dataset.\n"
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Further splitting the training data into training and validation sets (70:15:15 ratio)\n",
        "#X_train, X_val, y_train, y_val = train_test_split(X_train_resampled, y_train_resampled_df, test_size=0.2, random_state=0)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T03:43:45.192917Z",
          "iopub.execute_input": "2024-06-03T03:43:45.194161Z",
          "iopub.status.idle": "2024-06-03T03:43:45.200689Z",
          "shell.execute_reply.started": "2024-06-03T03:43:45.194115Z",
          "shell.execute_reply": "2024-06-03T03:43:45.199437Z"
        },
        "trusted": true,
        "id": "VCVQuxyrmGJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Describe info about train and test set\n",
        "print(\"Number of transactions in X_train dataset: \", X_train_resampled.shape)\n",
        "print(\"Number of transactions in y_train dataset: \", y_train_resampled_df.shape)\n",
        "print(\"Number of transactions in X_test dataset: \", X_test.shape)\n",
        "print(\"Number of transactions in y_test dataset: \", y_test.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T03:43:45.202406Z",
          "iopub.execute_input": "2024-06-03T03:43:45.203117Z",
          "iopub.status.idle": "2024-06-03T03:43:45.214383Z",
          "shell.execute_reply.started": "2024-06-03T03:43:45.203074Z",
          "shell.execute_reply": "2024-06-03T03:43:45.212721Z"
        },
        "trusted": true,
        "id": "oriSEsz8mGJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DL Model - 1 - **Deep Learning ANN Classification Model**"
      ],
      "metadata": {
        "id": "4CN3TwqAUlX6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1: Install Required Libraries**"
      ],
      "metadata": {
        "id": "_vZpizgSUzRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==2.15.0"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T03:43:45.216144Z",
          "iopub.execute_input": "2024-06-03T03:43:45.21683Z",
          "iopub.status.idle": "2024-06-03T03:44:01.121687Z",
          "shell.execute_reply.started": "2024-06-03T03:43:45.216723Z",
          "shell.execute_reply": "2024-06-03T03:44:01.120476Z"
        },
        "trusted": true,
        "id": "9VAYwVjVmGJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras"
      ],
      "metadata": {
        "id": "8wZsFm5OfgKp",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:44:01.123105Z",
          "iopub.execute_input": "2024-06-03T03:44:01.123435Z",
          "iopub.status.idle": "2024-06-03T03:44:20.263148Z",
          "shell.execute_reply.started": "2024-06-03T03:44:01.123407Z",
          "shell.execute_reply": "2024-06-03T03:44:20.261945Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "p564joBMXm1M",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:44:20.266927Z",
          "iopub.execute_input": "2024-06-03T03:44:20.267263Z",
          "iopub.status.idle": "2024-06-03T03:44:37.87825Z",
          "shell.execute_reply.started": "2024-06-03T03:44:20.267233Z",
          "shell.execute_reply": "2024-06-03T03:44:37.877309Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2: Import Libraries**"
      ],
      "metadata": {
        "id": "UAElNMhkU5LU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T03:44:37.879693Z",
          "iopub.execute_input": "2024-06-03T03:44:37.880849Z",
          "iopub.status.idle": "2024-06-03T03:44:37.912744Z",
          "shell.execute_reply.started": "2024-06-03T03:44:37.880811Z",
          "shell.execute_reply": "2024-06-03T03:44:37.911764Z"
        },
        "trusted": true,
        "id": "r4nwQs9PmGJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dense, BatchNormalization, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2oj6MlYMU9bC",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:44:37.913945Z",
          "iopub.execute_input": "2024-06-03T03:44:37.914306Z",
          "iopub.status.idle": "2024-06-03T03:44:37.927095Z",
          "shell.execute_reply.started": "2024-06-03T03:44:37.914272Z",
          "shell.execute_reply": "2024-06-03T03:44:37.926276Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3: Ensuring the target labels  in the correct format.**"
      ],
      "metadata": {
        "id": "AS6soD94mGJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure target labels are numerical and feature arrays are float\n",
        "y_train_numerical = y_train_resampled_df.astype(int)\n",
        "y_test_numerical = y_test.astype(int)\n",
        "\n",
        "# Convert DataFrame to numpy array and ensure float32 type\n",
        "X_train_array = X_train_resampled.values.astype(np.float32)\n",
        "X_test_array = X_test.values.astype(np.float32)\n",
        "\n",
        "# Ensure target labels are numpy arrays\n",
        "y_train_array = np.array(y_train_numerical)\n",
        "y_test_array = np.array(y_test_numerical)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T03:44:37.928226Z",
          "iopub.execute_input": "2024-06-03T03:44:37.928556Z",
          "iopub.status.idle": "2024-06-03T03:46:00.330364Z",
          "shell.execute_reply.started": "2024-06-03T03:44:37.928526Z",
          "shell.execute_reply": "2024-06-03T03:46:00.329298Z"
        },
        "trusted": true,
        "id": "0QmQl3ixmGJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get input dimensions\n",
        "input_dim = X_train.shape[1]\n",
        "num_classes = len(np.unique(y))\n",
        "input_dim ,num_classes"
      ],
      "metadata": {
        "id": "bqgSNNo0ZxVE",
        "execution": {
          "iopub.status.busy": "2024-06-03T03:46:00.331593Z",
          "iopub.execute_input": "2024-06-03T03:46:00.331957Z",
          "iopub.status.idle": "2024-06-03T03:46:00.339383Z",
          "shell.execute_reply.started": "2024-06-03T03:46:00.331932Z",
          "shell.execute_reply": "2024-06-03T03:46:00.338412Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 4: Define the ANN Model**"
      ],
      "metadata": {
        "id": "Dd2Wi_N-ZQxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Adding Learning Rate Scheduler**\n",
        "\n",
        "First, you need to define a learning rate scheduler function:"
      ],
      "metadata": {
        "id": "oZZxR3YSmGJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T03:46:00.34049Z",
          "iopub.execute_input": "2024-06-03T03:46:00.340778Z",
          "iopub.status.idle": "2024-06-03T03:46:00.354051Z",
          "shell.execute_reply.started": "2024-06-03T03:46:00.340756Z",
          "shell.execute_reply": "2024-06-03T03:46:00.353284Z"
        },
        "trusted": true,
        "id": "hLkv0PetmGJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import models\n",
        "\n",
        "# Dropout rate\n",
        "dropout_rate = 0.5\n",
        "\n",
        "# Define the neural network model with BatchNormalization and Dropout layers\n",
        "neural_classifier = Sequential(\n",
        "    [\n",
        "        Dense(128, activation=\"relu\", kernel_regularizer=l2(),input_dim=X_train.shape[1]),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "\n",
        "        Dense(96, activation=\"relu\", kernel_regularizer=l2()),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "\n",
        "        Dense(64, activation=\"relu\", kernel_regularizer=l2()),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "\n",
        "        Dense(32, activation=\"relu\", kernel_regularizer=l2()),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "\n",
        "        Dense(num_classes, activation=\"softmax\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Print the model summary\n",
        "neural_classifier.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T03:46:00.355275Z",
          "iopub.execute_input": "2024-06-03T03:46:00.355544Z",
          "iopub.status.idle": "2024-06-03T03:46:01.55864Z",
          "shell.execute_reply.started": "2024-06-03T03:46:00.355522Z",
          "shell.execute_reply": "2024-06-03T03:46:01.557802Z"
        },
        "trusted": true,
        "id": "xPCbBuZTmGJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 5: Define and Initialize the Keras Classifier model**"
      ],
      "metadata": {
        "id": "dI1MSTt4aLcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Initialize Model\n",
        "\n",
        "scikeras_classifier = KerasClassifier(model=neural_classifier,\n",
        "                                    optimizer=\"adam\",\n",
        "                                    loss=keras.losses.categorical_crossentropy,\n",
        "                                    batch_size=4000,\n",
        "                                    epochs=30,\n",
        "                                    metrics=['accuracy'],\n",
        "                                    random_state=42,\n",
        "                                    warm_start=True\n",
        "                          )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T03:46:01.55975Z",
          "iopub.execute_input": "2024-06-03T03:46:01.560028Z",
          "iopub.status.idle": "2024-06-03T03:46:01.564991Z",
          "shell.execute_reply.started": "2024-06-03T03:46:01.560004Z",
          "shell.execute_reply": "2024-06-03T03:46:01.564192Z"
        },
        "trusted": true,
        "id": "qo7yKPfNmGJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 6: Initialize StratifiedKFold Cross Validation (no. of folds=3)**"
      ],
      "metadata": {
        "id": "4Ouqsd43mGJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define number of folds\n",
        "n_folds = 3\n",
        "\n",
        "# Initialize StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T03:46:01.566175Z",
          "iopub.execute_input": "2024-06-03T03:46:01.566635Z",
          "iopub.status.idle": "2024-06-03T03:46:01.575879Z",
          "shell.execute_reply.started": "2024-06-03T03:46:01.566605Z",
          "shell.execute_reply": "2024-06-03T03:46:01.575019Z"
        },
        "trusted": true,
        "id": "tWyHXh8wmGJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 7: Performing 3-fold cross validation and training the ANN deep learning model**"
      ],
      "metadata": {
        "id": "7Pc5QlAamGJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists to store train and test accuracies\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "# Lists to store train and test accuracies for visualization\n",
        "history_list = []\n",
        "\n",
        "# Perform 3-fold cross-validation\n",
        "for train_index, test_index in skf.split(X_train_array, np.argmax(y_train_array, axis=1)):\n",
        "    X_train_fold, X_test_fold = X_train_array[train_index], X_train_array[test_index]\n",
        "    y_train_fold, y_test_fold = y_train_array[train_index], y_train_array[test_index]\n",
        "\n",
        "    # Define EarlyStopping callback\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "    # Define LearningRateScheduler callback\n",
        "    lr_scheduler = LearningRateScheduler(scheduler)\n",
        "\n",
        "    # Fit the model with early stopping and learning rate scheduler\n",
        "    scikeras_classifier.fit(X_train_fold, y_train_fold,\n",
        "              validation_data=(X_test_fold, y_test_fold),\n",
        "              callbacks=[early_stopping, lr_scheduler],\n",
        "              verbose=1)\n",
        "\n",
        "    # Append the history for visualization later\n",
        "    history_list.append(scikeras_classifier.history_)\n",
        "\n",
        "    # Evaluate the model on train data\n",
        "    train_accuracy = scikeras_classifier.score(X_train_fold, y_train_fold)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Evaluate the model on test data\n",
        "    test_accuracy = scikeras_classifier.score(X_test_fold, y_test_fold)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    # Train metric\n",
        "    y_pred_tr = scikeras_classifier.predict(X_train_fold)\n",
        "    y_pred_classes_tr = np.argmax(y_pred_tr, axis=1)\n",
        "    y_test_classes_tr = np.argmax(y_train_fold, axis=1)\n",
        "\n",
        "    # Test Metric\n",
        "    y_pred = scikeras_classifier.predict(X_test_fold)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_test_classes = np.argmax(y_test_fold, axis=1)\n",
        "\n",
        "    print(\"Train Accuracy:\", accuracy_score(y_test_classes_tr, y_pred_classes_tr))\n",
        "    print(\"Test Accuracy:\", accuracy_score(y_test_classes, y_pred_classes))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test_classes, y_pred_classes))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T03:46:01.577026Z",
          "iopub.execute_input": "2024-06-03T03:46:01.577398Z",
          "iopub.status.idle": "2024-06-03T03:53:52.772585Z",
          "shell.execute_reply.started": "2024-06-03T03:46:01.57737Z",
          "shell.execute_reply": "2024-06-03T03:53:52.771621Z"
        },
        "trusted": true,
        "id": "nZV2UMmAmGJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate mean train and test accuracies\n",
        "mean_train_accuracy = np.mean(train_accuracies)\n",
        "mean_test_accuracy = np.mean(test_accuracies)\n",
        "\n",
        "# Evaluation Metrics\n",
        "print(\"Mean Train Accuracy:\", mean_train_accuracy)\n",
        "print(\"Mean Test Accuracy:\", mean_test_accuracy)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T03:53:52.774231Z",
          "iopub.execute_input": "2024-06-03T03:53:52.775027Z",
          "iopub.status.idle": "2024-06-03T03:53:52.780388Z",
          "shell.execute_reply.started": "2024-06-03T03:53:52.774989Z",
          "shell.execute_reply": "2024-06-03T03:53:52.779542Z"
        },
        "trusted": true,
        "id": "nMBU-NpZmGJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step-8 Grid Search Model Hyperparameters**\n"
      ],
      "metadata": {
        "id": "eFpS8dJxmGJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "params = {\n",
        "    \"optimizer__learning_rate\": [0.01, 0.001],\n",
        "    \"epochs\":[19,30],\n",
        "    \"batch_size\": [32,64,4000],\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(scikeras_classifier, params, scoring='accuracy')\n",
        "\n",
        "grid.fit(X_train, y_train)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T03:53:52.781477Z",
          "iopub.execute_input": "2024-06-03T03:53:52.781777Z",
          "iopub.status.idle": "2024-06-03T05:25:40.879832Z",
          "shell.execute_reply.started": "2024-06-03T03:53:52.781753Z",
          "shell.execute_reply": "2024-06-03T05:25:40.87881Z"
        },
        "trusted": true,
        "id": "sSbm4_oZmGJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best Score  : {}\".format(grid.best_score_))\n",
        "print(\"Best Params : {}\".format(grid.best_params_))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T05:25:40.881252Z",
          "iopub.execute_input": "2024-06-03T05:25:40.881718Z",
          "iopub.status.idle": "2024-06-03T05:25:40.887018Z",
          "shell.execute_reply.started": "2024-06-03T05:25:40.881684Z",
          "shell.execute_reply": "2024-06-03T05:25:40.886155Z"
        },
        "trusted": true,
        "id": "aBBZtCGamGJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 9: Evaluating performance of ANN deep learning model**"
      ],
      "metadata": {
        "id": "u84LfJIgmGJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Evaluate Model\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "print(\"Train Accuracy : {}\".format(accuracy_score(y_train, grid.predict(X_train))))\n",
        "print(\"Test  Accuracy : {}\".format(accuracy_score(y_test, grid.predict(X_test))))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T05:25:40.888236Z",
          "iopub.execute_input": "2024-06-03T05:25:40.888521Z",
          "iopub.status.idle": "2024-06-03T05:25:58.395607Z",
          "shell.execute_reply.started": "2024-06-03T05:25:40.888498Z",
          "shell.execute_reply": "2024-06-03T05:25:58.394618Z"
        },
        "trusted": true,
        "id": "s2umOTqjmGJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 10: Visualization the performance of ANN deep learning model**\n",
        "Analyze the model's predictions to identify trends, patterns, and areas for service improvement.\n"
      ],
      "metadata": {
        "id": "aNJbu3NymGJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ROC AUC Curve**"
      ],
      "metadata": {
        "id": "pz6HHHN0mGJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "def plot_roc_curve(y_test, y_pred, num_classes):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i in range(num_classes):\n",
        "        fpr, tpr, _ = roc_curve(y_test[:, i], y_pred[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, lw=2, label='Class %d (area = %0.2f)' % (i, roc_auc))\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "# Assuming `y_test_classes` and `y_pred` are the one-hot encoded true and predicted labels\n",
        "plot_roc_curve(y_test_fold, y_pred, num_classes)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T05:25:58.397146Z",
          "iopub.execute_input": "2024-06-03T05:25:58.397889Z",
          "iopub.status.idle": "2024-06-03T05:25:58.788958Z",
          "shell.execute_reply.started": "2024-06-03T05:25:58.397851Z",
          "shell.execute_reply": "2024-06-03T05:25:58.787981Z"
        },
        "trusted": true,
        "id": "N9MznO3XmGJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Confusion Matrix**"
      ],
      "metadata": {
        "id": "HJOIInvlmGJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_test, y_pred_classes, class_names):\n",
        "    cm = confusion_matrix(np.argmax(y_test, axis=1), y_pred_classes)\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Assuming `y_test_classes` and `y_pred_classes` are the true and predicted labels\n",
        "class_names = ['Class 1', 'Class 2', 'Class 3','Class 4','Class 5']  # Replace with your actual class names\n",
        "plot_confusion_matrix(y_test_fold, y_pred_classes, class_names)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T05:25:58.790144Z",
          "iopub.execute_input": "2024-06-03T05:25:58.790447Z",
          "iopub.status.idle": "2024-06-03T05:25:59.125442Z",
          "shell.execute_reply.started": "2024-06-03T05:25:58.790421Z",
          "shell.execute_reply": "2024-06-03T05:25:59.124541Z"
        },
        "trusted": true,
        "id": "wKxBuo5OmGJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "l0s4jBCBmGJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Classification Report**"
      ],
      "metadata": {
        "id": "smWkx9YQmGJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_classification_report(y_test, y_pred_classes, class_names):\n",
        "    class_names = ['Class 1', 'Class 2', 'Class 3','Class 4','Class 5']\n",
        "    report = classification_report(np.argmax(y_test, axis=1), y_pred_classes, target_names=class_names)\n",
        "    print(\"Classification Report:\\n\", report)\n",
        "\n",
        "# Print classification report\n",
        "print_classification_report(y_test_fold, y_pred_classes, class_names)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T05:25:59.126893Z",
          "iopub.execute_input": "2024-06-03T05:25:59.127272Z",
          "iopub.status.idle": "2024-06-03T05:25:59.162419Z",
          "shell.execute_reply.started": "2024-06-03T05:25:59.127238Z",
          "shell.execute_reply": "2024-06-03T05:25:59.161587Z"
        },
        "trusted": true,
        "id": "IxCOomtRmGJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training and Validation Accuracy Plot**"
      ],
      "metadata": {
        "id": "2YMu4OtzmGJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming history_list contains the training history for each fold\n",
        "mean_train_accuracy = []\n",
        "mean_val_accuracy = []\n",
        "\n",
        "# Calculate mean accuracy for each epoch\n",
        "for epoch in range(30):  # Assuming max 30 epochs\n",
        "    epoch_train_acc = np.mean([history['accuracy'][epoch] for history in history_list if epoch < len(history['accuracy'])])\n",
        "    epoch_val_acc = np.mean([history['val_accuracy'][epoch] for history in history_list if epoch < len(history['val_accuracy'])])\n",
        "    mean_train_accuracy.append(epoch_train_acc)\n",
        "    mean_val_accuracy.append(epoch_val_acc)\n",
        "\n",
        "# Plot mean train and validation accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(mean_train_accuracy, label='Mean Train Accuracy')\n",
        "plt.plot(mean_val_accuracy, label='Mean Validation Accuracy')\n",
        "plt.title('Mean Training and Validation Accuracy Across Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T05:25:59.163504Z",
          "iopub.execute_input": "2024-06-03T05:25:59.163798Z",
          "iopub.status.idle": "2024-06-03T05:25:59.489877Z",
          "shell.execute_reply.started": "2024-06-03T05:25:59.163773Z",
          "shell.execute_reply": "2024-06-03T05:25:59.488963Z"
        },
        "trusted": true,
        "id": "u8cu8D5SmGJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing Blog**\n"
      ],
      "metadata": {
        "id": "eLTSyMe3_MXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/almabetter/data-preprocessing-ea09fac6a7f7"
      ],
      "metadata": {
        "id": "XYNmUe2H_pPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Here are some solution to Reduce Customer Churn\n",
        "\n",
        "* Modify International Plan as the charge is same as normal one.\n",
        "* Be proactive with communication.\n",
        "* Ask for feedback often.\n",
        "* Periodically throw Offers to retain customers.\n",
        "* Look at the customers facing problem in the most churning states.\n",
        "* Lean into best customers.\n",
        "* Regular Server Maintenance.\n",
        "* Solving Poor Network Connectivity Issue.\n",
        "* Define a roadmap for new customers.\n",
        "* Analyze churn when it happens.\n",
        "* Stay competitive.\n",
        "\n",
        "2. The four charge fields are linear functions of the minute fields.\n",
        "\n",
        "3. The area code field and/or the state field are anomalous, and can be omitted.\n",
        "\n",
        "4. Customers with the International Plan tend to churn more frequently.\n",
        "\n",
        "5. Customers with four or more customer service calls churn more than four times as often as do the other customers.\n",
        "\n",
        "6. Customers with high day minutes and evening minutes tend to churn at a higher rate than do the other customers.\n",
        "\n",
        "7. There is no obvious association of churn with the variables day calls, evening calls, night calls, international calls, night minutes, international minutes, account length, or voice mail messages.\n",
        "\n",
        "8. We can deploy the model with XGBoost algorithm. Because For training dataset, i found precision of 100% and recall of 91% and f1-score of 95% for False Churn customer data. BUt, I am also interested to see the result for Churning cutomer result as I got precision of 46% and recall of 95% and f1-score of 62%. Accuracy is 92% and average percision, recall & f1_score are 73%, 93% and 79% respectively with a roc auc score of 72%. For testing dataset, i found precision of 99% and recall of 90% and f1-score of 94% for False Churn customer data. BUt, I am also interested to see the result for Churning cutomer result as I got precision of 35% and recall of 81% and f1-score of 49%. Accuracy is 90% and average percision, recall & f1_score are 67%, 86% and 72% respectively with a roc auc score of 66%. It's the best performing model i found.\n",
        "\n",
        "9. No overfitting is seen.\n",
        "\n",
        "10. Due to less no. of data in the dataset, the scores are around 80%. Once we get more data we can retrain our algorithm for better performance."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Deep Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scikeras_classifier.model_.save(\"csat_model.h5\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T05:25:59.491091Z",
          "iopub.execute_input": "2024-06-03T05:25:59.491452Z",
          "iopub.status.idle": "2024-06-03T05:25:59.569954Z",
          "shell.execute_reply.started": "2024-06-03T05:25:59.491421Z",
          "shell.execute_reply": "2024-06-03T05:25:59.56894Z"
        },
        "trusted": true,
        "id": "mD0qCISYmGJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7BWGX5VAmGJt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}